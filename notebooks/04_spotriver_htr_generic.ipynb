{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"River Hyperparameter Tuning with SPOT HTR\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Parameter Optimization\n",
    "## `river` Hyperparameter Tuning: HTR with User Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip list | grep spot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install --upgrade build\n",
    "# !{sys.executable} -m pip install --upgrade --force-reinstall http://www.gm.fh-koeln.de/~bartz/site/spotPython.tar.gz"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: HTR Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import inf\n",
    "from spotRiver.fun.hyperriver import HyperRiver\n",
    "from spotPython.spot import spot\n",
    "from spotRiver import data\n",
    "from scipy.optimize import shgo\n",
    "from scipy.optimize import direct\n",
    "from scipy.optimize import differential_evolution\n",
    "import matplotlib.pyplot as plt\n",
    "from spotRiver.utils.selectors import select_leaf_prediction\n",
    "from spotRiver.utils.selectors import select_leaf_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 The Objective Function \n",
    "\n",
    "* Here we will use the river `HATR` function.\n",
    "* First, the function will be tested independently from `Spot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime as dt\n",
    "from river import datasets\n",
    "from river import time_series\n",
    "from river import utils\n",
    "import calendar\n",
    "import math\n",
    "from river import compose\n",
    "from river import linear_model\n",
    "from river import optim\n",
    "from river import preprocessing\n",
    "from river import metrics\n",
    "from spotRiver.fun.hyperriver import HyperRiver\n",
    "\n",
    "fun = HyperRiver(123).fun_HTR_iter_progressive\n",
    "var_type = [\"int\"] * 2 + [\"num\"] * 2 + [\"factor\"] * 2 + [\"float\"] + [\"int\"] * 2 + [\"factor\"] + [\"float\"]\n",
    "var_type\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note: Instead of using the Airline Passenger data as in notebook 01, we demonstrate the usage of user specified data which is *not* part of the `spotRiver` package!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Specified Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspotstream.datasets import fetch_opm\n",
    "# ds = fetch_opm(include_categorical=False, data_home=\"data\", return_X_y=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Important: Specify the location of your data, e.g., `/home/data/` etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotRiver.data.generic import GenericData\n",
    "fraction = 1.0\n",
    "dataset = GenericData(filename=\"opm_cat.csv\",\n",
    "                      directory=\".\",\n",
    "                      target=\"Sale Amount\",\n",
    "                      n_features=14,\n",
    "                      n_samples=985_862,\n",
    "                      fraction = fraction,\n",
    "                      converters={'List Year': int,\n",
    "                                  'Assessed Value': float,\n",
    "                                  'Sale Amount': float,\n",
    "                                  'Sales Ratio': float,\n",
    "                                  'lon': float,\n",
    "                                  'lat': float,\n",
    "                                  'timestamp_rec': float},\n",
    "                      parse_dates=None\n",
    "                      # parse_dates={\"Date Recorded\": \"%Y-%m-%d\"}\n",
    "                      )\n",
    "n_samples = int(dataset.n_samples * fraction)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECK Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from river.datasets import synth\n",
    "# dataset = synth.Friedman(seed=42).take(100_000)\n",
    "# n_samples = 500_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in dataset:\n",
    "    print(x,y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [[2.00e+02 1.00e+01 1.00e-06 7.50e-02 2.00e+00 1.00e+00 0.00e+00 7.00e+00 0.00e+00 3.12e+02]]\n",
    "lower = np.array([100, 1,  1e-8, 0.025, 0, 1, 0.8,  0,  5, 0, 250.0])\n",
    "upper = np.array([200, 10, 1e-6, 0.075, 2, 1, 0.975, 1, 10, 0, 750.0])\n",
    "fun_control = {\"data\": dataset,\n",
    "               \"n_samples\": n_samples}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Run the `Spot` Optimizer\n",
    "\n",
    "* Since the data is larger than the airline passengers data, the max. time is increased."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Errors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This setting fails:\n",
    "\n",
    "grace_period 105\n",
    "max_depth 1000\n",
    "delta 9.752288533728755e-07\n",
    "tau 0.0691657548549048\n",
    "leaf_prediction adaptive\n",
    "leaf_model Perceptron\n",
    "model_selector_decay 1.0\n",
    "splitter EBSTSplitter\n",
    "min_samples_split 5\n",
    "binary_split 0\n",
    "max_size 564.0\n",
    "Eval iter_prog_val_score: [#---------] 14.20% \n",
    "\n",
    "\n",
    "with the following error messages:\n",
    "\n",
    "grace_period 193\n",
    "max_depth 10000\n",
    "delta 5.160737490382046e-07\n",
    "tau 0.030631256378602662\n",
    "leaf_prediction adaptive\n",
    "leaf_model Perceptron\n",
    "model_selector_decay 1.0\n",
    "splitter QOSplitter\n",
    "min_samples_split 7\n",
    "binary_split 0\n",
    "max_size 354.0\n",
    "Eval iter_prog_val_score: [##########] 100.00% Done...\n",
    "grace_period 136\n",
    "max_depth 10\n",
    "delta 4.358994017976757e-07\n",
    "tau 0.06735151680780518\n",
    "leaf_prediction mean\n",
    "leaf_model Perceptron\n",
    "model_selector_decay 1.0\n",
    "splitter QOSplitter\n",
    "min_samples_split 10\n",
    "binary_split 0\n",
    "max_size 385.0\n",
    "Eval iter_prog_val_score: [##########] 100.00% Done...\n",
    "grace_period 101\n",
    "max_depth 100000000\n",
    "delta 6.740303972572442e-07\n",
    "tau 0.056668192013518505\n",
    "leaf_prediction adaptive\n",
    "leaf_model Perceptron\n",
    "model_selector_decay 1.0\n",
    "splitter EBSTSplitter\n",
    "min_samples_split 6\n",
    "binary_split 0\n",
    "max_size 721.0\n",
    "Eval iter_prog_val_score: [##########] 100.00% Done...\n",
    "grace_period 105\n",
    "max_depth 1000\n",
    "delta 9.752288533728755e-07\n",
    "tau 0.0691657548549048\n",
    "leaf_prediction adaptive\n",
    "leaf_model Perceptron\n",
    "model_selector_decay 1.0\n",
    "splitter EBSTSplitter\n",
    "min_samples_split 5\n",
    "binary_split 0\n",
    "max_size 564.0\n",
    "Eval iter_prog_val_score: [#---------] 14.20% \n",
    "Output exceeds the size limit. Open the full output data in a text editor\n",
    "---------------------------------------------------------------------------\n",
    "RecursionError                            Traceback (most recent call last)\n",
    "Cell In [10], line 27\n",
    "      1 spot_htr = spot.Spot(fun=fun,\n",
    "      2                    lower = lower,\n",
    "      3                    upper = upper,\n",
    "   (...)\n",
    "     25                                       \"model_fun_evals\": 1000,\n",
    "     26                                       })\n",
    "---> 27 spot_htr.run()\n",
    "\n",
    "File ~/miniforge3/envs/spotCondaEnv/lib/python3.10/site-packages/spotPython/spot/spot.py:290, in Spot.run(self)\n",
    "    288 else:\n",
    "    289     X_all = X0\n",
    "--> 290 self.y = self.fun(X=X_all, fun_control=self.fun_control)\n",
    "    291 self.X, self.y = remove_nan(self.X, self.y)\n",
    "    292 self.update_stats()\n",
    "\n",
    "File ~/miniforge3/envs/spotCondaEnv/lib/python3.10/site-packages/spotRiver/fun/hyperriver.py:399, in HyperRiver.fun_HTR_iter_progressive(self, X, fun_control)\n",
    "    397 # cat = compose.SelectType(str) | preprocessing.OneHotEncoder()\n",
    "    398 cat = compose.SelectType(str) | preprocessing.FeatureHasher(n_features=1000, seed=1)\n",
    "--> 399 res = eval_oml_iter_progressive(\n",
    "    400     dataset=self.fun_control[\"data\"],\n",
    "    401     step=10000,\n",
    "    402     verbose=True,\n",
    "    403     metric=metrics.MAE(),\n",
    "    404     models={\n",
    "    405         \"HTR\": (\n",
    "    406             (num + cat)\n",
    "    407             | tree.HoeffdingTreeRegressor(\n",
    "    408                 grace_period=int(grace_period[i]),\n",
    "    409                 max_depth=select_max_depth(int(max_depth[i])),\n",
    "    410                 delta=float(delta[i]),\n",
    "    411                 tau=float(tau[i]),\n",
    "    412                 leaf_prediction=select_leaf_prediction(int(leaf_prediction[i])),\n",
    "    413                 leaf_model=select_leaf_model(int(leaf_model[i])),\n",
    "    414                 model_selector_decay=float(model_selector_decay[i]),\n",
    "    415                 splitter=select_splitter(int(splitter[i])),\n",
    "    416                 min_samples_split=int(min_samples_split[i]),\n",
    "    417                 binary_split=int(binary_split[i]),\n",
    "    418                 max_size=float(max_size[i])\n",
    "    419             )\n",
    "    420         ),\n",
    "    421     },\n",
    "    422 )\n",
    "    423 y = fun_eval_oml_iter_progressive(res, metric=None)\n",
    "    424 z_res = np.append(z_res, y / self.fun_control[\"n_samples\"])\n",
    "\n",
    "File ~/miniforge3/envs/spotCondaEnv/lib/python3.10/site-packages/spotRiver/evaluation/eval_oml.py:28, in eval_oml_iter_progressive(dataset, metric, models, step, verbose)\n",
    "     26 for model_name, model in models.items():\n",
    "     27     result_i = {\"step\": [], \"error\": [], \"r_time\": [], \"memory\": []}\n",
    "---> 28     for checkpoint in iter_progressive_val_score(\n",
    "     29         dataset, model, metric, measure_time=True, measure_memory=True, step=step\n",
    "     30     ):\n",
    "     31         if verbose:\n",
    "     32             progress_bar(checkpoint[\"Step\"] / n_steps, message=\"Eval iter_prog_val_score:\")\n",
    "\n",
    "File ~/miniforge3/envs/spotCondaEnv/lib/python3.10/site-packages/river/evaluate/progressive_validation.py:190, in iter_progressive_val_score(dataset, model, metric, moment, delay, step, measure_time, measure_memory)\n",
    "    102 def iter_progressive_val_score(\n",
    "    103     dataset: base.typing.Dataset,\n",
    "    104     model,\n",
    "   (...)\n",
    "    110     measure_memory=False,\n",
    "    111 ) -> typing.Generator:\n",
    "    112     \"\"\"Evaluates the performance of a model on a streaming dataset and yields results.\n",
    "    113 \n",
    "    114     This does exactly the same as `evaluate.progressive_val_score`. The only difference is that\n",
    "   (...)\n",
    "    187 \n",
    "    188     \"\"\"\n",
    "--> 190     yield from _progressive_validation(\n",
    "    191         dataset,\n",
    "    192         model,\n",
    "    193         metric,\n",
    "    194         checkpoints=itertools.count(step, step) if step else iter([]),\n",
    "    195         moment=moment,\n",
    "    196         delay=delay,\n",
    "    197         measure_time=measure_time,\n",
    "    198         measure_memory=measure_memory,\n",
    "    199     )\n",
    "\n",
    "File ~/miniforge3/envs/spotCondaEnv/lib/python3.10/site-packages/river/evaluate/progressive_validation.py:86, in _progressive_validation(dataset, model, metric, checkpoints, moment, delay, measure_time, measure_memory)\n",
    "     84 n_samples_learned += 1\n",
    "     85 if model._supervised:\n",
    "---> 86     model.learn_one(x, y, **kwargs)\n",
    "     87 else:\n",
    "     88     model.learn_one(x, **kwargs)\n",
    "\n",
    "File ~/miniforge3/envs/spotCondaEnv/lib/python3.10/site-packages/river/compose/pipeline.py:533, in Pipeline.learn_one(self, x, y, **params)\n",
    "    531 last_step = next(steps)\n",
    "    532 if last_step._supervised:\n",
    "--> 533     last_step.learn_one(x=x, y=y, **params)\n",
    "    534 else:\n",
    "    535     last_step.learn_one(x, **params)\n",
    "...\n",
    "--> 137 d = id(x)\n",
    "    138 y = memo.get(d, _nil)\n",
    "    139 if y is not _nil:\n",
    "\n",
    "RecursionError: maximum recursion depth exceeded while calling a Python object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_htr = spot.Spot(fun=fun,\n",
    "                   lower = lower,\n",
    "                   upper = upper,\n",
    "                   fun_evals = inf,\n",
    "                   fun_repeats = 1,\n",
    "                   max_time = 300,\n",
    "                   noise = False,\n",
    "                   tolerance_x = np.sqrt(np.spacing(1)),\n",
    "                   var_type=var_type,\n",
    "                   infill_criterion = \"y\",\n",
    "                   n_points = 1,\n",
    "                   seed=123,\n",
    "                   log_level = 50,\n",
    "                   show_models= False,\n",
    "                   show_progress= True,\n",
    "                   fun_control = fun_control,\n",
    "                   design_control={\"init_size\": 25,\n",
    "                                   \"repeats\": 1},\n",
    "                   surrogate_control={\"noise\": False,\n",
    "                                      \"cod_type\": \"norm\",\n",
    "                                      \"min_theta\": -4,\n",
    "                                      \"max_theta\": 3,\n",
    "                                      \"n_theta\": 11,\n",
    "                                      \"model_optimizer\": differential_evolution,\n",
    "                                      \"model_fun_evals\": 1000,\n",
    "                                      })\n",
    "spot_htr.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Results \n",
    "\n",
    "* Print the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_htr.print_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Show the Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_htr.plot_progress(log_y=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_htr.print_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = spot_htr.k\n",
    "for i in range(n-1):\n",
    "    for j in range(i+1, n):\n",
    "        spot_htr.plot_contour(i=i, j=j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Evaluate SNARIMAX Model with Tuned Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = spot_htr.to_all_dim(spot_htr.min_X.reshape(1,-1))\n",
    "print(X)\n",
    "grace_period = X[:, 0]\n",
    "max_depth = X[:, 1]\n",
    "delta = X[:, 2]\n",
    "tau = X[:, 3]\n",
    "leaf_prediction = X[:, 4]\n",
    "leaf_model = X[:, 5]\n",
    "model_selector_decay = X[:, 6]\n",
    "splitter = X[:, 7]\n",
    "min_samples_split = X[:, 8]\n",
    "binary_split = X[:, 9]\n",
    "max_size = X[:, 10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numbers\n",
    "from river import tree\n",
    "from spotRiver.evaluation.eval_oml import fun_eval_oml_iter_progressive\n",
    "from spotRiver.evaluation.eval_oml import eval_oml_iter_progressive\n",
    "from spotRiver.utils.selectors import select_splitter\n",
    "from spotRiver.utils.selectors import select_max_depth\n",
    "num = compose.SelectType(numbers.Number) | preprocessing.StandardScaler()\n",
    "cat = compose.SelectType(str) | preprocessing.FeatureHasher(n_features=1000, seed=1)\n",
    "res = eval_oml_iter_progressive(\n",
    "    dataset=fun_control[\"data\"],\n",
    "    step=10000,\n",
    "    verbose=True,\n",
    "    metric=metrics.MAE(),\n",
    "    models={\n",
    "         \"Default: HTR + QO\": (\n",
    "             (num + cat) | tree.HoeffdingTreeRegressor(\n",
    "                splitter=tree.splitter.QOSplitter()\n",
    "            )\n",
    "        ),\n",
    "        \"SPOT: HTR + QO\": (\n",
    "            (num + cat) | tree.HoeffdingTreeRegressor(\n",
    "                grace_period=int(grace_period),\n",
    "                max_depth=select_max_depth(int(max_depth)),\n",
    "                delta=float(delta),\n",
    "                tau=float(tau),\n",
    "                leaf_prediction=select_leaf_prediction(int(leaf_prediction)),\n",
    "                leaf_model=select_leaf_model(int(leaf_model)),\n",
    "                splitter=select_splitter(int(splitter)),\n",
    "                min_samples_split=int(min_samples_split),\n",
    "                binary_split=int(binary_split),\n",
    "                max_size=float(max_size)\n",
    "            )\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "y = fun_eval_oml_iter_progressive(res, metric=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotRiver.evaluation.eval_oml import plot_oml_iter_progressive\n",
    "plot_oml_iter_progressive(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "# model[\"ts\"].regressor[\"lm\"].weights\n",
    "# forecast = model.forecast(horizon=fun_control[\"horizon\"])\n",
    "# forecast\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spotCondaEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "81c77de872def749acd68d9955e19f0df6803301f4c1f66c3444af66334112ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

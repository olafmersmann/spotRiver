{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"River Hyperparameter Tuning with SPOT HTR\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Parameter Optimization\n",
    "## `river` Hyperparameter Tuning: HTR with User Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspot                                    0.0.1\n",
      "pyspotriver                               0.0.3\n",
      "pyspotstream                              0.2.0\n",
      "spotPython                                0.0.8\n",
      "spotRiver                                 0.0.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list | grep spot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install --upgrade build\n",
    "# !{sys.executable} -m pip install --upgrade --force-reinstall http://www.gm.fh-koeln.de/~bartz/site/spotPython.tar.gz"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: HTR Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import inf\n",
    "from spotRiver.fun.hyperriver import HyperRiver\n",
    "from spotPython.spot import spot\n",
    "from spotRiver import data\n",
    "from scipy.optimize import shgo\n",
    "from scipy.optimize import direct\n",
    "from scipy.optimize import differential_evolution\n",
    "import matplotlib.pyplot as plt\n",
    "from spotRiver.utils.selectors import select_leaf_prediction\n",
    "from spotRiver.utils.selectors import select_leaf_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 The Objective Function \n",
    "\n",
    "* Here we will use the river `HATR` function.\n",
    "* First, the function will be tested independently from `Spot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['int',\n",
       " 'int',\n",
       " 'num',\n",
       " 'num',\n",
       " 'factor',\n",
       " 'factor',\n",
       " 'float',\n",
       " 'int',\n",
       " 'int',\n",
       " 'factor',\n",
       " 'float']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import datetime as dt\n",
    "from river import datasets\n",
    "from river import time_series\n",
    "from river import utils\n",
    "import calendar\n",
    "import math\n",
    "from river import compose\n",
    "from river import linear_model\n",
    "from river import optim\n",
    "from river import preprocessing\n",
    "from river import metrics\n",
    "from spotRiver.fun.hyperriver import HyperRiver\n",
    "\n",
    "fun = HyperRiver(123).fun_HTR_iter_progressive\n",
    "var_type = [\"int\"] * 2 + [\"num\"] * 2 + [\"factor\"] * 2 + [\"float\"] + [\"int\"] * 2 + [\"factor\"] + [\"float\"]\n",
    "var_type\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note: Instead of using the Airline Passenger data as in notebook 01, we demonstrate the usage of user specified data which is *not* part of the `spotRiver` package!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Specified Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspotstream.datasets import fetch_opm\n",
    "# ds = fetch_opm(include_categorical=False, data_home=\"data\", return_X_y=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Important: Specify the location of your data, e.g., `/home/data/` etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotRiver.data.generic import GenericData\n",
    "fraction = 0.1\n",
    "dataset = GenericData(filename=\"opm_cat.csv\",\n",
    "                      directory=\".\",\n",
    "                      target=\"Sale Amount\",\n",
    "                      n_features=14,\n",
    "                      n_samples=985_862,\n",
    "                      fraction = fraction,\n",
    "                      converters={'List Year': int,\n",
    "                                  'Assessed Value': float,\n",
    "                                  'Sale Amount': float,\n",
    "                                  'Sales Ratio': float,\n",
    "                                  'lon': float,\n",
    "                                  'lat': float,\n",
    "                                  'timestamp_rec': float},\n",
    "                      parse_dates=None\n",
    "                      # parse_dates={\"Date Recorded\": \"%Y-%m-%d\"}\n",
    "                      )\n",
    "n_samples = int(dataset.n_samples * fraction)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECK Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from river.datasets import synth\n",
    "# dataset = synth.Friedman(seed=42).take(100_000)\n",
    "# n_samples = 500_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'List Year': 2001, 'Assessed Value': 107530.0, 'Sales Ratio': 0.575026738, 'lat': 41.50021401170393, 'lon': -72.87396263711757, 'timestamp_rec': 1001894400.0, 'Town': 'Bristol', 'Address': \"MULTI #'S CONSTANCE LN\", 'Property Type': 'Unknown', 'Residential Type': 'Unknown', 'Non Use Code': 'Unknown', 'Assessor Remarks': 'Unknown', 'OPM remarks': 'Unknown'} 187000.0\n"
     ]
    }
   ],
   "source": [
    "for x,y in dataset:\n",
    "    print(x,y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [[2.00e+02 1.00e+01 1.00e-06 7.50e-02 2.00e+00 1.00e+00 0.00e+00 7.00e+00 0.00e+00 3.12e+02]]\n",
    "lower = np.array([100, 1,  1e-8, 0.025, 0, 0, 0.8,  0,  5, 0, 250.0])\n",
    "upper = np.array([200, 10, 1e-6, 0.075, 2, 1, 0.975, 1, 10, 1, 750.0])\n",
    "fun_control = {\"data\": dataset,\n",
    "               \"n_samples\": n_samples}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Run the `Spot` Optimizer\n",
    "\n",
    "* Since the data is larger than the airline passengers data, the max. time is increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grace_period 183\n",
      "max_depth 100\n",
      "delta 4.831843725955114e-07\n",
      "tau 0.06407814094650666\n",
      "leaf_prediction adaptive\n",
      "leaf_model LinearRegression\n",
      "model_selector_decay 1.0\n",
      "splitter EBSTSplitter\n",
      "min_samples_split 6\n",
      "binary_split 1\n",
      "max_size 624.0\n",
      "Eval iter_prog_val_score: [##########] 100.00% Done...\n",
      "grace_period 178\n",
      "max_depth 1000\n",
      "delta 8.783746662586555e-08\n",
      "tau 0.051292664738826446\n",
      "leaf_prediction model\n",
      "leaf_model Perceptron\n",
      "model_selector_decay 1.0\n",
      "splitter EBSTSplitter\n",
      "min_samples_split 10\n",
      "binary_split 0\n",
      "max_size 392.0\n",
      "Eval iter_prog_val_score: [##########] 100.00% Done...\n",
      "grace_period 125\n",
      "max_depth 100000\n",
      "delta 6.847505392445806e-07\n",
      "tau 0.06992552541619884\n",
      "leaf_prediction model\n",
      "leaf_model Perceptron\n",
      "model_selector_decay 1.0\n",
      "splitter QOSplitter\n",
      "min_samples_split 8\n",
      "binary_split 1\n",
      "max_size 739.0\n",
      "Eval iter_prog_val_score: [##########] 100.00% Done...\n",
      "grace_period 131\n",
      "max_depth 100\n",
      "delta 8.734913362604061e-07\n",
      "tau 0.03601478223408268\n",
      "leaf_prediction adaptive\n",
      "leaf_model LinearRegression\n",
      "model_selector_decay 1.0\n",
      "splitter EBSTSplitter\n",
      "min_samples_split 8\n",
      "binary_split 0\n",
      "max_size 412.0\n",
      "Eval iter_prog_val_score: [##########] 100.00% Done...\n",
      "grace_period 118\n",
      "max_depth 100000000\n",
      "delta 6.025670267977835e-07\n",
      "tau 0.059837240391967056\n",
      "leaf_prediction model\n",
      "leaf_model Perceptron\n",
      "model_selector_decay 1.0\n",
      "splitter QOSplitter\n",
      "min_samples_split 7\n",
      "binary_split 1\n",
      "max_size 521.0\n",
      "Eval iter_prog_val_score: [##########] 100.00% Done...\n",
      "grace_period 196\n",
      "max_depth 10000\n",
      "delta 3.4496736611039124e-07\n",
      "tau 0.04866958043994363\n",
      "leaf_prediction mean\n",
      "leaf_model Perceptron\n",
      "model_selector_decay 1.0\n",
      "splitter QOSplitter\n",
      "min_samples_split 9\n",
      "binary_split 0\n",
      "max_size 349.0\n",
      "Eval iter_prog_val_score: [##########] 100.00% Done...\n",
      "grace_period 105\n",
      "max_depth 1000000\n",
      "delta 7.083347713199366e-07\n",
      "tau 0.027823369599172566\n",
      "leaf_prediction adaptive\n",
      "leaf_model LinearRegression\n",
      "model_selector_decay 1.0\n",
      "splitter QOSplitter\n",
      "min_samples_split 5\n",
      "binary_split 1\n",
      "max_size 669.0\n",
      "Eval iter_prog_val_score: [##########] 100.00% Done...\n",
      "grace_period 163\n",
      "max_depth 10000000\n",
      "delta 9.700225888604838e-07\n",
      "tau 0.04497054983341265\n",
      "leaf_prediction adaptive\n",
      "leaf_model LinearRegression\n",
      "model_selector_decay 1.0\n",
      "splitter EBSTSplitter\n",
      "min_samples_split 6\n",
      "binary_split 0\n",
      "max_size 584.0\n",
      "Eval iter_prog_val_score: [##########] 100.00% Done...\n",
      "grace_period 151\n",
      "max_depth 1000000000\n",
      "delta 2.2660090105915595e-07\n",
      "tau 0.03100578200970443\n",
      "leaf_prediction mean\n",
      "leaf_model Perceptron\n",
      "model_selector_decay 1.0\n",
      "splitter QOSplitter\n",
      "min_samples_split 7\n",
      "binary_split 1\n",
      "max_size 491.0\n",
      "Eval iter_prog_val_score: [##########] 100.00% Done...\n",
      "grace_period 150\n",
      "max_depth 1000000000\n",
      "delta 1.6554818091815006e-07\n",
      "tau 0.07070701289220323\n",
      "leaf_prediction mean\n",
      "leaf_model LinearRegression\n",
      "model_selector_decay 1.0\n",
      "splitter EBSTSplitter\n",
      "min_samples_split 9\n",
      "binary_split 0\n",
      "max_size 293.0\n",
      "Eval iter_prog_val_score: [##########] 100.00% Done...\n",
      "grace_period 100\n",
      "max_depth 10\n",
      "delta 1e-08\n",
      "tau 0.025\n",
      "leaf_prediction adaptive\n",
      "leaf_model LinearRegression\n",
      "model_selector_decay 1.0\n",
      "splitter EBSTSplitter\n",
      "min_samples_split 10\n",
      "binary_split 0\n",
      "max_size 684.0\n",
      "Eval iter_prog_val_score: [##########] 100.00% Done...\n",
      "grace_period 100g: [#---------] 6.20% \n",
      "max_depth 10\n",
      "delta 1e-06\n",
      "tau 0.075\n",
      "leaf_prediction adaptive\n",
      "leaf_model Perceptron\n",
      "model_selector_decay 1.0\n",
      "splitter EBSTSplitter\n",
      "min_samples_split 9\n",
      "binary_split 1\n",
      "max_size 661.0\n",
      "Eval iter_prog_val_score: [##########] 100.00% Done...\n",
      "grace_period 105g: [#---------] 11.90% \n",
      "max_depth 10\n",
      "delta 1e-08\n",
      "tau 0.025\n",
      "leaf_prediction adaptive\n",
      "leaf_model Perceptron\n",
      "model_selector_decay 1.0\n",
      "splitter EBSTSplitter\n",
      "min_samples_split 10\n",
      "binary_split 0\n",
      "max_size 587.0\n",
      "Eval iter_prog_val_score: [##########] 100.00% Done...\n",
      "grace_period 100g: [##--------] 18.21% \n",
      "max_depth 100\n",
      "delta 1e-06\n",
      "tau 0.025\n",
      "leaf_prediction adaptive\n",
      "leaf_model LinearRegression\n",
      "model_selector_decay 1.0\n",
      "splitter EBSTSplitter\n",
      "min_samples_split 10\n",
      "binary_split 0\n",
      "max_size 353.0\n",
      "Eval iter_prog_val_score: [##########] 100.00% Done...\n",
      "grace_period 100g: [###-------] 29.06% \n",
      "max_depth 100\n",
      "delta 1e-06\n",
      "tau 0.025\n",
      "leaf_prediction adaptive\n",
      "leaf_model LinearRegression\n",
      "model_selector_decay 1.0\n",
      "splitter EBSTSplitter\n",
      "min_samples_split 10\n",
      "binary_split 1\n",
      "max_size 507.0\n",
      "Eval iter_prog_val_score: [#####-----] 50.73% \r"
     ]
    }
   ],
   "source": [
    "spot_htr = spot.Spot(fun=fun,\n",
    "                   lower = lower,\n",
    "                   upper = upper,\n",
    "                   fun_evals = inf,\n",
    "                   fun_repeats = 1,\n",
    "                   max_time = 10,\n",
    "                   noise = False,\n",
    "                   tolerance_x = np.sqrt(np.spacing(1)),\n",
    "                   var_type=var_type,\n",
    "                   infill_criterion = \"y\",\n",
    "                   n_points = 1,\n",
    "                   seed=123,\n",
    "                   log_level = 50,\n",
    "                   show_models= False,\n",
    "                   show_progress= True,\n",
    "                   fun_control = fun_control,\n",
    "                   design_control={\"init_size\": 10,\n",
    "                                   \"repeats\": 1},\n",
    "                   surrogate_control={\"noise\": False,\n",
    "                                      \"cod_type\": \"norm\",\n",
    "                                      \"min_theta\": -4,\n",
    "                                      \"max_theta\": 3,\n",
    "                                      \"n_theta\": 11,\n",
    "                                      \"model_optimizer\": differential_evolution,\n",
    "                                      \"model_fun_evals\": 1000,\n",
    "                                      })\n",
    "spot_htr.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Results \n",
    "\n",
    "* Print the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_htr.print_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Show the Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_htr.plot_progress(log_y=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_htr.print_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = spot_htr.k\n",
    "for i in range(n-1):\n",
    "    for j in range(i+1, n):\n",
    "        spot_htr.plot_contour(i=i, j=j)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Evaluate HTR Model with Tuned Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = spot_htr.to_all_dim(spot_htr.min_X.reshape(1,-1))\n",
    "print(X)\n",
    "grace_period = X[:, 0]\n",
    "max_depth = X[:, 1]\n",
    "delta = X[:, 2]\n",
    "tau = X[:, 3]\n",
    "leaf_prediction = X[:, 4]\n",
    "leaf_model = X[:, 5]\n",
    "model_selector_decay = X[:, 6]\n",
    "splitter = X[:, 7]\n",
    "min_samples_split = X[:, 8]\n",
    "binary_split = X[:, 9]\n",
    "max_size = X[:, 10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numbers\n",
    "from river import tree\n",
    "from spotRiver.evaluation.eval_oml import fun_eval_oml_iter_progressive\n",
    "from spotRiver.evaluation.eval_oml import eval_oml_iter_progressive\n",
    "from spotRiver.utils.selectors import select_splitter\n",
    "from spotRiver.utils.selectors import select_max_depth\n",
    "num = compose.SelectType(numbers.Number) | preprocessing.StandardScaler()\n",
    "cat = compose.SelectType(str) | preprocessing.FeatureHasher(n_features=1000, seed=1)\n",
    "res = eval_oml_iter_progressive(\n",
    "    dataset=fun_control[\"data\"],\n",
    "    step=10000,\n",
    "    verbose=True,\n",
    "    metric=metrics.MAE(),\n",
    "    models={\n",
    "         \"Default: HTR + QO\": (\n",
    "             (num + cat) | tree.HoeffdingTreeRegressor(\n",
    "                splitter=tree.splitter.QOSplitter()\n",
    "            )\n",
    "        ),\n",
    "        \"SPOT: HTR + QO\": (\n",
    "            (num + cat) | tree.HoeffdingTreeRegressor(\n",
    "                grace_period=int(grace_period),\n",
    "                max_depth=select_max_depth(int(max_depth)),\n",
    "                delta=float(delta),\n",
    "                tau=float(tau),\n",
    "                leaf_prediction=select_leaf_prediction(int(leaf_prediction)),\n",
    "                leaf_model=select_leaf_model(int(leaf_model)),\n",
    "                splitter=select_splitter(int(splitter)),\n",
    "                min_samples_split=int(min_samples_split),\n",
    "                binary_split=int(binary_split),\n",
    "                max_size=float(max_size)\n",
    "            )\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "y = fun_eval_oml_iter_progressive(res, metric=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotRiver.evaluation.eval_oml import plot_oml_iter_progressive\n",
    "plot_oml_iter_progressive(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "# model[\"ts\"].regressor[\"lm\"].weights\n",
    "# forecast = model.forecast(horizon=fun_control[\"horizon\"])\n",
    "# forecast\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spotCondaEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "81c77de872def749acd68d9955e19f0df6803301f4c1f66c3444af66334112ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

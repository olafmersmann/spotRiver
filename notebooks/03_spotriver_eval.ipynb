{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"River Hyperparameter Tuning with SPOT Evaluation\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Parameter Optimization\n",
    "## `river` Hyperparameter Tuning: Evaluation\n",
    "\n",
    "See: [https://riverml.xyz/0.15.0/examples/batch-to-online/](https://riverml.xyz/0.15.0/examples/batch-to-online/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip list | grep spot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install --upgrade build\n",
    "# !{sys.executable} -m pip install --upgrade --force-reinstall spotRiver"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# Load the data\n",
    "dataset = datasets.load_breast_cancer()\n",
    "X, y = dataset.data, dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspotstream.datasets import fetch_opm\n",
    "ds = fetch_opm(include_categorical=False, data_home=\"data\", return_X_y=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Batch Machine Learning with Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn import pipeline\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# Define the steps of the model\n",
    "sk_model = pipeline.Pipeline([\n",
    "    ('scale', preprocessing.StandardScaler()),\n",
    "    ('lin_reg', linear_model.LogisticRegression(solver='lbfgs'))\n",
    "])\n",
    "\n",
    "# Define a deterministic cross-validation procedure\n",
    "cv = model_selection.KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Compute the MSE values\n",
    "scorer = metrics.make_scorer(metrics.roc_auc_score)\n",
    "scores = model_selection.cross_val_score(sk_model, X, y, scoring=scorer, cv=cv)\n",
    "\n",
    "# Display the average score and it's standard deviation\n",
    "print(f'ROC AUC: {scores.mean():.3f} (± {scores.std():.3f})')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Batch Machine Learning with River's `compat` Wrapper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can train a river model, e.g., LogisticRegression, in an OML manner:\n",
    "  * the model is trained on single instances and not on the whole data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import linear_model\n",
    "from river import compat\n",
    "from river import compose\n",
    "from river import preprocessing\n",
    "\n",
    "# We define a Pipeline, exactly like we did earlier for sklearn \n",
    "rv_model = compose.Pipeline(\n",
    "    ('scale', preprocessing.StandardScaler()),\n",
    "    ('log_reg', linear_model.LogisticRegression())\n",
    ")\n",
    "\n",
    "# We make the Pipeline compatible with sklearn\n",
    "# learn_one is called for each observation\n",
    "rv2sk_model = compat.convert_river_to_sklearn(rv_model)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we can apply sklearn's methods to the wrapped river model\n",
    "* We compute the CV scores using the same CV scheme and the same scoring\n",
    " 1. If sklearn's fit is called, then:\n",
    "   Fit with one pass of the dataset portion is called\n",
    "   learn_one is called for each observation\n",
    " 2. If sklearn's predict s called, then:\n",
    "   A prediction is made for each observation, the same (fitted) model is used,\n",
    "   i.e., the model is not updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scores = model_selection.cross_val_score(rv2sk_model, X, y, scoring=scorer, cv=cv)\n",
    "\n",
    "# Display the average score and it's standard deviation\n",
    "print(f'ROC AUC: {scores.mean():.3f} (± {scores.std():.3f})')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Mini Batch for River Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import linear_model\n",
    "from river import compose\n",
    "from river import preprocessing\n",
    "from river import stream\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "rv_model = compose.Pipeline(\n",
    "    ('scale', preprocessing.StandardScaler()),\n",
    "    ('lin_reg', linear_model.Perceptron())\n",
    ")\n",
    "\n",
    "names = [\"Assessed Value\", \"Sale Amount\", \"Sales Ratio\"]\n",
    "\n",
    "for x in pd.read_csv(\"data/opm_2001-2020.csv\", usecols=names, chunksize=int(985862/10), nrows=985862, header=0):\n",
    "    y = x.pop(\"Assessed Value\")\n",
    "    y_pred = rv_model.predict_proba_many(x)\n",
    "    rv_model.learn_many(x, y)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Performance for HTs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* See [https://riverml.xyz/0.15.0/recipes/on-hoeffding-trees/](https://riverml.xyz/0.15.0/recipes/on-hoeffding-trees/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "\n",
    "from river import datasets\n",
    "from river import evaluate\n",
    "from river import metrics\n",
    "from river import preprocessing  # we are going to use that later\n",
    "from river.datasets import synth  # we are going to use some synthetic datasets too\n",
    "from river import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance(dataset, metric, models):\n",
    "    metric_name = metric.__class__.__name__\n",
    "\n",
    "    # To make the generated data reusable\n",
    "    dataset = list(dataset)\n",
    "    fig, ax = plt.subplots(figsize=(10, 5), nrows=3, dpi=300)\n",
    "    for model_name, model in models.items():\n",
    "        step = []\n",
    "        error = []\n",
    "        r_time = []\n",
    "        memory = []\n",
    "\n",
    "        for checkpoint in evaluate.iter_progressive_val_score(\n",
    "            dataset, model, metric, measure_time=True, measure_memory=True, step=100\n",
    "        ):\n",
    "            step.append(checkpoint[\"Step\"])\n",
    "            error.append(checkpoint[metric_name].get())\n",
    "\n",
    "            # Convert timedelta object into seconds\n",
    "            r_time.append(checkpoint[\"Time\"].total_seconds())\n",
    "            # Make sure the memory measurements are in MB\n",
    "            raw_memory = checkpoint[\"Memory\"]\n",
    "            memory.append(raw_memory * 2**-20)\n",
    "\n",
    "        ax[0].plot(step, error, label=model_name)\n",
    "        ax[1].plot(step, r_time, label=model_name)\n",
    "        ax[2].plot(step, memory, label=model_name)\n",
    "\n",
    "    ax[0].set_ylabel(metric_name)\n",
    "    ax[1].set_ylabel('Time (seconds)')\n",
    "    ax[2].set_ylabel('Memory (MB)')\n",
    "    ax[2].set_xlabel('Instances')\n",
    "\n",
    "    ax[0].grid(True)\n",
    "    ax[1].grid(True)\n",
    "    ax[2].grid(True)\n",
    "\n",
    "    ax[0].legend(\n",
    "        loc='upper center', bbox_to_anchor=(0.5, 1.25),\n",
    "        ncol=3, fancybox=True, shadow=True\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.close()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(\n",
    "    dataset = synth.Friedman(seed=42).take(10_000),\n",
    "    metric = metrics.MAE(),\n",
    "    models =\n",
    "    {\n",
    "        \"Unbounded HTR\": (\n",
    "            preprocessing.StandardScaler() |\n",
    "            tree.HoeffdingTreeRegressor(splitter=tree.splitter.EBSTSplitter())\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval_OML from spotRiver"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The OPM Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from spotRiver.data.opm import fetch_opm\n",
    "df = fetch_opm(include_categorical=True, data_home=\"data\", return_df=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [\"Location\", \"lon\", \"lat\"]:\n",
    "    df.dropna(subset=[i], inplace=True)\n",
    "df.reset_index(inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\n",
    "            \"Town\",\n",
    "            \"Address\",\n",
    "            \"Property Type\",\n",
    "            \"Residential Type\",\n",
    "            \"Non Use Code\",\n",
    "            \"Assessor Remarks\",\n",
    "            \"OPM remarks\",\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=categorical_columns, axis=1)\n",
    "df = df.drop(columns=[\"Location\"], axis = 1)\n",
    "df = df.drop(columns=[\"Date Recorded\"], axis = 1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"opm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotRiver.data.generic import GenericData\n",
    "opm_dataset = GenericData(filename=\"opm.csv\",\n",
    "                      directory=\".\",\n",
    "                      target=\"Sale Amount\",\n",
    "                      n_features=7,\n",
    "                      n_samples=195_832,\n",
    "                      converters={'index': int,\n",
    "                                  'Serial Number': int,\n",
    "                                  'List Year': int,\n",
    "                                  'Assessed Value': float,\n",
    "                                  'Sale Amount': float,\n",
    "                                  'Sales Ratio': float,\n",
    "                                  'lon': float,\n",
    "                                  'lat': float,\n",
    "                                  'timestamp_rec': float},\n",
    "                      parse_dates=None\n",
    "                      # parse_dates={\"Date Recorded\": \"%Y-%m-%d\"}\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in opm_dataset:\n",
    "    print(x,y)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation with KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "hash_list = list([\"Town\", \"Address\", \"Property Type\",\n",
    "       \"Residential Type\", \"Non Use Code\", \"Assessor Remarks\", \"OPM remarks\"])\n",
    "for i in hash_list:\n",
    "    df[i + \"hash\"] = pd.DataFrame(map(lambda x: str(hash(x)), df[i]))\n",
    "df_num = df.drop(columns=hash_list)\n",
    "df_num = df_num.drop(columns=[\"timestamp_rec\", \"Date Recorded\", \"Location\"])\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "df_imp = imputer.fit_transform(df_num) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" from river import stream\n",
    "params = {\n",
    "     'converters': {'rating': float},\n",
    "     'parse_dates': {'year': '%Y'}\n",
    "}\n",
    "\n",
    "dataset = stream.iter_csv('tv_shows.csv', target='rating', **params)\n",
    "for x, y in dataset:\n",
    "    print(x, y) \"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The GW Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotRiver.data.generic import GenericData\n",
    "dataset = GenericData(filename=\"UnivariateData.csv\",\n",
    "                      directory=\"/Users/bartz/data/\",\n",
    "                      target=\"Consumption\",\n",
    "                      n_features=1,\n",
    "                      n_samples=51_706,\n",
    "                      converters={\"Consumption\": float},\n",
    "                      parse_dates={\"Time\": \"%Y-%m-%d %H:%M:%S%z\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in dataset:\n",
    "    print(x,y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "\n",
    "from river import datasets\n",
    "from river import evaluate\n",
    "from river import metrics\n",
    "from river import preprocessing  # we are going to use that later\n",
    "from river.datasets import synth  # we are going to use some synthetic datasets too\n",
    "from river import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotRiver.evaluation.eval_oml import eval_oml_iter_progressive\n",
    "from spotRiver.evaluation.eval_oml import plot_oml_iter_progressive"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TODO: Change dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = opm_dataset  # synth.Friedman(seed=42).take(10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_1 = eval_oml_iter_progressive(\n",
    "    dataset = dataset,\n",
    "    step = 10000,\n",
    "    verbose = True,\n",
    "    metric = metrics.MAE(),\n",
    "    models =\n",
    "    {\n",
    "        \"HTR + E-BST\": (\n",
    "            preprocessing.StandardScaler() | tree.HoeffdingTreeRegressor(\n",
    "                splitter=tree.splitter.EBSTSplitter()\n",
    "            )\n",
    "        ),\n",
    "        \"HTR + TE-BST\": (\n",
    "            preprocessing.StandardScaler() | tree.HoeffdingTreeRegressor(\n",
    "                splitter=tree.splitter.TEBSTSplitter()\n",
    "            )\n",
    "        ),\n",
    "        \"HTR + QO\": (\n",
    "            preprocessing.StandardScaler() | tree.HoeffdingTreeRegressor(\n",
    "                splitter=tree.splitter.QOSplitter()\n",
    "            )\n",
    "        ),\n",
    "\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_oml_iter_progressive(res_1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPM 2nd Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(985862, 14)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from spotRiver.data.opm import fetch_opm\n",
    "X, y = fetch_opm(include_categorical=True, data_home=\"data\", return_X_y=True)\n",
    "X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "List Year                0\n",
       "Assessed Value           0\n",
       "Sale Amount              0\n",
       "Sales Ratio              0\n",
       "lat                 790030\n",
       "lon                 790030\n",
       "timestamp_rec            0\n",
       "Town                     0\n",
       "Address                  0\n",
       "Property Type            0\n",
       "Residential Type         0\n",
       "Non Use Code             0\n",
       "Assessor Remarks         0\n",
       "OPM remarks              0\n",
       "Sale Amount              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([X, y], axis=1)\n",
    "df.isnull().sum()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute Mean for lat ans lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.lat = SimpleImputer().fit_transform(np.array(df.lat).reshape(-1,1))\n",
    "df.lon = SimpleImputer().fit_transform(np.array(df.lon).reshape(-1,1))\n",
    "# df = pd.DataFrame(SimpleImputer().fit_transform(df), columns = df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "List Year           0\n",
       "Assessed Value      0\n",
       "Sale Amount         0\n",
       "Sales Ratio         0\n",
       "lat                 0\n",
       "lon                 0\n",
       "timestamp_rec       0\n",
       "Town                0\n",
       "Address             0\n",
       "Property Type       0\n",
       "Residential Type    0\n",
       "Non Use Code        0\n",
       "Assessor Remarks    0\n",
       "OPM remarks         0\n",
       "Sale Amount         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>List Year</th>\n",
       "      <th>Assessed Value</th>\n",
       "      <th>Sale Amount</th>\n",
       "      <th>Sales Ratio</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>timestamp_rec</th>\n",
       "      <th>Town</th>\n",
       "      <th>Address</th>\n",
       "      <th>Property Type</th>\n",
       "      <th>Residential Type</th>\n",
       "      <th>Non Use Code</th>\n",
       "      <th>Assessor Remarks</th>\n",
       "      <th>OPM remarks</th>\n",
       "      <th>Sale Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001</td>\n",
       "      <td>107530</td>\n",
       "      <td>187000.0</td>\n",
       "      <td>0.575027</td>\n",
       "      <td>41.500214</td>\n",
       "      <td>-72.873963</td>\n",
       "      <td>1.001894e+09</td>\n",
       "      <td>Bristol</td>\n",
       "      <td>MULTI #'S CONSTANCE LN</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>187000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>47000</td>\n",
       "      <td>71900.0</td>\n",
       "      <td>0.653686</td>\n",
       "      <td>41.500214</td>\n",
       "      <td>-72.873963</td>\n",
       "      <td>1.001894e+09</td>\n",
       "      <td>Bridgeport</td>\n",
       "      <td>333 VINCELLETTE ST 33</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>71900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001</td>\n",
       "      <td>50720</td>\n",
       "      <td>57750.0</td>\n",
       "      <td>0.878268</td>\n",
       "      <td>41.500214</td>\n",
       "      <td>-72.873963</td>\n",
       "      <td>1.001894e+09</td>\n",
       "      <td>New Britain</td>\n",
       "      <td>135 GOLD ST</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>57750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001</td>\n",
       "      <td>80010</td>\n",
       "      <td>115000.0</td>\n",
       "      <td>0.695739</td>\n",
       "      <td>41.867160</td>\n",
       "      <td>-72.441560</td>\n",
       "      <td>1.001894e+09</td>\n",
       "      <td>Vernon</td>\n",
       "      <td>114 E MAIN ST</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>115000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001</td>\n",
       "      <td>152800</td>\n",
       "      <td>315000.0</td>\n",
       "      <td>0.485079</td>\n",
       "      <td>41.500214</td>\n",
       "      <td>-72.873963</td>\n",
       "      <td>1.001894e+09</td>\n",
       "      <td>Orange</td>\n",
       "      <td>809 ROBERT TREAT DR</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>315000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   List Year  Assessed Value  Sale Amount  Sales Ratio        lat        lon  \\\n",
       "0       2001          107530     187000.0     0.575027  41.500214 -72.873963   \n",
       "1       2001           47000      71900.0     0.653686  41.500214 -72.873963   \n",
       "2       2001           50720      57750.0     0.878268  41.500214 -72.873963   \n",
       "3       2001           80010     115000.0     0.695739  41.867160 -72.441560   \n",
       "4       2001          152800     315000.0     0.485079  41.500214 -72.873963   \n",
       "\n",
       "   timestamp_rec         Town                 Address Property Type  \\\n",
       "0   1.001894e+09      Bristol  MULTI #'S CONSTANCE LN       Unknown   \n",
       "1   1.001894e+09   Bridgeport   333 VINCELLETTE ST 33       Unknown   \n",
       "2   1.001894e+09  New Britain             135 GOLD ST       Unknown   \n",
       "3   1.001894e+09       Vernon           114 E MAIN ST       Unknown   \n",
       "4   1.001894e+09       Orange     809 ROBERT TREAT DR       Unknown   \n",
       "\n",
       "  Residential Type Non Use Code Assessor Remarks OPM remarks  Sale Amount  \n",
       "0          Unknown      Unknown          Unknown     Unknown     187000.0  \n",
       "1          Unknown      Unknown          Unknown     Unknown      71900.0  \n",
       "2          Unknown      Unknown          Unknown     Unknown      57750.0  \n",
       "3          Unknown      Unknown          Unknown     Unknown     115000.0  \n",
       "4          Unknown      Unknown          Unknown     Unknown     315000.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spotCondaEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "81c77de872def749acd68d9955e19f0df6803301f4c1f66c3444af66334112ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

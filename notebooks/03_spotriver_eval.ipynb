{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"River Hyperparameter Tuning with SPOT Evaluation\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Parameter Optimization\n",
    "## `river` Hyperparameter Tuning: Evaluation\n",
    "\n",
    "See: [https://riverml.xyz/0.15.0/examples/batch-to-online/](https://riverml.xyz/0.15.0/examples/batch-to-online/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip list | grep spot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install --upgrade build\n",
    "# !{sys.executable} -m pip install --upgrade --force-reinstall spotRiver"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# Load the data\n",
    "dataset = datasets.load_breast_cancer()\n",
    "X, y = dataset.data, dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspotstream.datasets import fetch_opm\n",
    "ds = fetch_opm(include_categorical=False, data_home=\"data\", return_X_y=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Batch Machine Learning with Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn import pipeline\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# Define the steps of the model\n",
    "sk_model = pipeline.Pipeline([\n",
    "    ('scale', preprocessing.StandardScaler()),\n",
    "    ('lin_reg', linear_model.LogisticRegression(solver='lbfgs'))\n",
    "])\n",
    "\n",
    "# Define a deterministic cross-validation procedure\n",
    "cv = model_selection.KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Compute the MSE values\n",
    "scorer = metrics.make_scorer(metrics.roc_auc_score)\n",
    "scores = model_selection.cross_val_score(sk_model, X, y, scoring=scorer, cv=cv)\n",
    "\n",
    "# Display the average score and it's standard deviation\n",
    "print(f'ROC AUC: {scores.mean():.3f} (± {scores.std():.3f})')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Batch Machine Learning with River's `compat` Wrapper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can train a river model, e.g., LogisticRegression, in an OML manner:\n",
    "  * the model is trained on single instances and not on the whole data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import linear_model\n",
    "from river import compat\n",
    "from river import compose\n",
    "from river import preprocessing\n",
    "\n",
    "# We define a Pipeline, exactly like we did earlier for sklearn \n",
    "rv_model = compose.Pipeline(\n",
    "    ('scale', preprocessing.StandardScaler()),\n",
    "    ('log_reg', linear_model.LogisticRegression())\n",
    ")\n",
    "\n",
    "# We make the Pipeline compatible with sklearn\n",
    "# learn_one is called for each observation\n",
    "rv2sk_model = compat.convert_river_to_sklearn(rv_model)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we can apply sklearn's methods to the wrapped river model\n",
    "* We compute the CV scores using the same CV scheme and the same scoring\n",
    " 1. If sklearn's fit is called, then:\n",
    "   Fit with one pass of the dataset portion is called\n",
    "   learn_one is called for each observation\n",
    " 2. If sklearn's predict s called, then:\n",
    "   A prediction is made for each observation, the same (fitted) model is used,\n",
    "   i.e., the model is not updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scores = model_selection.cross_val_score(rv2sk_model, X, y, scoring=scorer, cv=cv)\n",
    "\n",
    "# Display the average score and it's standard deviation\n",
    "print(f'ROC AUC: {scores.mean():.3f} (± {scores.std():.3f})')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Mini Batch for River Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import linear_model\n",
    "from river import compose\n",
    "from river import preprocessing\n",
    "from river import stream\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "rv_model = compose.Pipeline(\n",
    "    ('scale', preprocessing.StandardScaler()),\n",
    "    ('lin_reg', linear_model.Perceptron())\n",
    ")\n",
    "\n",
    "names = [\"Assessed Value\", \"Sale Amount\", \"Sales Ratio\"]\n",
    "\n",
    "for x in pd.read_csv(\"data/opm_2001-2020.csv\", usecols=names, chunksize=int(985862/10), nrows=985862, header=0):\n",
    "    y = x.pop(\"Assessed Value\")\n",
    "    y_pred = rv_model.predict_proba_many(x)\n",
    "    rv_model.learn_many(x, y)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Performance for HTs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* See [https://riverml.xyz/0.15.0/recipes/on-hoeffding-trees/](https://riverml.xyz/0.15.0/recipes/on-hoeffding-trees/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "\n",
    "from river import datasets\n",
    "from river import evaluate\n",
    "from river import metrics\n",
    "from river import preprocessing  # we are going to use that later\n",
    "from river.datasets import synth  # we are going to use some synthetic datasets too\n",
    "from river import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance(dataset, metric, models):\n",
    "    metric_name = metric.__class__.__name__\n",
    "\n",
    "    # To make the generated data reusable\n",
    "    dataset = list(dataset)\n",
    "    fig, ax = plt.subplots(figsize=(10, 5), nrows=3, dpi=300)\n",
    "    for model_name, model in models.items():\n",
    "        step = []\n",
    "        error = []\n",
    "        r_time = []\n",
    "        memory = []\n",
    "\n",
    "        for checkpoint in evaluate.iter_progressive_val_score(\n",
    "            dataset, model, metric, measure_time=True, measure_memory=True, step=100\n",
    "        ):\n",
    "            step.append(checkpoint[\"Step\"])\n",
    "            error.append(checkpoint[metric_name].get())\n",
    "\n",
    "            # Convert timedelta object into seconds\n",
    "            r_time.append(checkpoint[\"Time\"].total_seconds())\n",
    "            # Make sure the memory measurements are in MB\n",
    "            raw_memory = checkpoint[\"Memory\"]\n",
    "            memory.append(raw_memory * 2**-20)\n",
    "\n",
    "        ax[0].plot(step, error, label=model_name)\n",
    "        ax[1].plot(step, r_time, label=model_name)\n",
    "        ax[2].plot(step, memory, label=model_name)\n",
    "\n",
    "    ax[0].set_ylabel(metric_name)\n",
    "    ax[1].set_ylabel('Time (seconds)')\n",
    "    ax[2].set_ylabel('Memory (MB)')\n",
    "    ax[2].set_xlabel('Instances')\n",
    "\n",
    "    ax[0].grid(True)\n",
    "    ax[1].grid(True)\n",
    "    ax[2].grid(True)\n",
    "\n",
    "    ax[0].legend(\n",
    "        loc='upper center', bbox_to_anchor=(0.5, 1.25),\n",
    "        ncol=3, fancybox=True, shadow=True\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.close()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance(\n",
    "    dataset = synth.Friedman(seed=42).take(10_000),\n",
    "    metric = metrics.MAE(),\n",
    "    models =\n",
    "    {\n",
    "        \"Unbounded HTR\": (\n",
    "            preprocessing.StandardScaler() |\n",
    "            tree.HoeffdingTreeRegressor(splitter=tree.splitter.EBSTSplitter())\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval_OML from spotRiver"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The OPM Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(985862, 17)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from spotRiver.data.opm import fetch_opm\n",
    "df = fetch_opm(include_categorical=True, data_home=\"data\", return_df=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Serial Number            0\n",
       "List Year                0\n",
       "Date Recorded            0\n",
       "Town                     0\n",
       "Address                  0\n",
       "Assessed Value           0\n",
       "Sale Amount              0\n",
       "Sales Ratio              0\n",
       "Property Type            0\n",
       "Residential Type         0\n",
       "Non Use Code             0\n",
       "Assessor Remarks         0\n",
       "OPM remarks              0\n",
       "Location            790018\n",
       "lon                 790030\n",
       "lat                 790030\n",
       "timestamp_rec            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(195832, 18)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in [\"Location\", \"lon\", \"lat\"]:\n",
    "    df.dropna(subset=[i], inplace=True)\n",
    "df.reset_index(inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index               0\n",
       "Serial Number       0\n",
       "List Year           0\n",
       "Date Recorded       0\n",
       "Town                0\n",
       "Address             0\n",
       "Assessed Value      0\n",
       "Sale Amount         0\n",
       "Sales Ratio         0\n",
       "Property Type       0\n",
       "Residential Type    0\n",
       "Non Use Code        0\n",
       "Assessor Remarks    0\n",
       "OPM remarks         0\n",
       "Location            0\n",
       "lon                 0\n",
       "lat                 0\n",
       "timestamp_rec       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\n",
    "            \"Town\",\n",
    "            \"Address\",\n",
    "            \"Property Type\",\n",
    "            \"Residential Type\",\n",
    "            \"Non Use Code\",\n",
    "            \"Assessor Remarks\",\n",
    "            \"OPM remarks\",\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(195832, 9)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(columns=categorical_columns, axis=1)\n",
    "df = df.drop(columns=[\"Location\"], axis = 1)\n",
    "df = df.drop(columns=[\"Date Recorded\"], axis = 1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'Serial Number', 'List Year', 'Assessed Value', 'Sale Amount',\n",
       "       'Sales Ratio', 'lon', 'lat', 'timestamp_rec'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"opm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotRiver.data.generic import GenericData\n",
    "opm_dataset = GenericData(filename=\"opm.csv\",\n",
    "                      directory=\".\",\n",
    "                      target=\"Sale Amount\",\n",
    "                      n_features=7,\n",
    "                      n_samples=195_832,\n",
    "                      converters={'index': int,\n",
    "                                  'Serial Number': int,\n",
    "                                  'List Year': int,\n",
    "                                  'Assessed Value': float,\n",
    "                                  'Sale Amount': float,\n",
    "                                  'Sales Ratio': float,\n",
    "                                  'lon': float,\n",
    "                                  'lat': float,\n",
    "                                  'timestamp_rec': float},\n",
    "                      parse_dates=None\n",
    "                      # parse_dates={\"Date Recorded\": \"%Y-%m-%d\"}\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index': 3, 'Serial Number': 10010, 'List Year': 2001, 'Assessed Value': 80010.0, 'Sales Ratio': 0.69573913, 'lon': -72.44156, 'lat': 41.86716, 'timestamp_rec': 1001894400.0} 115000.0\n"
     ]
    }
   ],
   "source": [
    "for x,y in opm_dataset:\n",
    "    print(x,y)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation with KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "hash_list = list([\"Town\", \"Address\", \"Property Type\",\n",
    "       \"Residential Type\", \"Non Use Code\", \"Assessor Remarks\", \"OPM remarks\"])\n",
    "for i in hash_list:\n",
    "    df[i + \"hash\"] = pd.DataFrame(map(lambda x: str(hash(x)), df[i]))\n",
    "df_num = df.drop(columns=hash_list)\n",
    "df_num = df_num.drop(columns=[\"timestamp_rec\", \"Date Recorded\", \"Location\"])\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "df_imp = imputer.fit_transform(df_num) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" from river import stream\n",
    "params = {\n",
    "     'converters': {'rating': float},\n",
    "     'parse_dates': {'year': '%Y'}\n",
    "}\n",
    "\n",
    "dataset = stream.iter_csv('tv_shows.csv', target='rating', **params)\n",
    "for x, y in dataset:\n",
    "    print(x, y) \"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The GW Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotRiver.data.generic import GenericData\n",
    "dataset = GenericData(filename=\"UnivariateData.csv\",\n",
    "                      directory=\"/Users/bartz/data/\",\n",
    "                      target=\"Consumption\",\n",
    "                      n_features=1,\n",
    "                      n_samples=51_706,\n",
    "                      converters={\"Consumption\": float},\n",
    "                      parse_dates={\"Time\": \"%Y-%m-%d %H:%M:%S%z\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index': '3', 'Serial Number': '10010', 'List Year': '2001', 'Date Recorded': datetime.datetime(2001, 10, 1, 0, 0), 'Assessed Value': '80010', 'Sales Ratio': 0.69573913, 'lon': '-72.44156', 'lat': '41.86716', 'timestamp_rec': '1001894400.0'} 115000.0\n"
     ]
    }
   ],
   "source": [
    "for x,y in dataset:\n",
    "    print(x,y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "\n",
    "from river import datasets\n",
    "from river import evaluate\n",
    "from river import metrics\n",
    "from river import preprocessing  # we are going to use that later\n",
    "from river.datasets import synth  # we are going to use some synthetic datasets too\n",
    "from river import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotRiver.evaluation.eval_oml import eval_oml_iter_progressive\n",
    "from spotRiver.evaluation.eval_oml import plot_oml_iter_progressive"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TODO: Change dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = opm_dataset  # synth.Friedman(seed=42).take(10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_1 = eval_oml_iter_progressive(\n",
    "    dataset = dataset,\n",
    "    metric = metrics.MAE(),\n",
    "    models =\n",
    "    {\n",
    "        \"HTR + E-BST\": (\n",
    "            preprocessing.StandardScaler() | tree.HoeffdingTreeRegressor(\n",
    "                splitter=tree.splitter.EBSTSplitter()\n",
    "            )\n",
    "        ),\n",
    "        \"HTR + TE-BST\": (\n",
    "            preprocessing.StandardScaler() | tree.HoeffdingTreeRegressor(\n",
    "                splitter=tree.splitter.TEBSTSplitter()\n",
    "            )\n",
    "        ),\n",
    "        \"HTR + QO\": (\n",
    "            preprocessing.StandardScaler() | tree.HoeffdingTreeRegressor(\n",
    "                splitter=tree.splitter.QOSplitter()\n",
    "            )\n",
    "        ),\n",
    "\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_oml_iter_progressive(res_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spotCondaEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "81c77de872def749acd68d9955e19f0df6803301f4c1f66c3444af66334112ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"River Hyperparameter Tuning with SPOT HTR\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Parameter Optimization\n",
    "## `river` Hyperparameter Tuning: HTR with User Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspot                                    0.0.1\n",
      "pyspotriver                               0.0.3\n",
      "pyspotstream                              0.2.0\n",
      "spotPython                                0.0.7\n",
      "spotRiver                                 0.0.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list | grep spot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install --upgrade build\n",
    "# !{sys.executable} -m pip install --upgrade --force-reinstall http://www.gm.fh-koeln.de/~bartz/site/spotPython.tar.gz"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: HTR Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import inf\n",
    "from spotRiver.fun.hyperriver import HyperRiver\n",
    "from spotPython.spot import spot\n",
    "from spotRiver import data\n",
    "from scipy.optimize import shgo\n",
    "from scipy.optimize import direct\n",
    "from scipy.optimize import differential_evolution\n",
    "import matplotlib.pyplot as plt\n",
    "from spotRiver.utils.selectors import select_leaf_prediction\n",
    "from spotRiver.utils.selectors import select_leaf_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 The Objective Function \n",
    "\n",
    "* Here we will use the river `HATR` function.\n",
    "* First, the function will be tested independently from `Spot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['int',\n",
       " 'int',\n",
       " 'num',\n",
       " 'num',\n",
       " 'factor',\n",
       " 'factor',\n",
       " 'float',\n",
       " 'int',\n",
       " 'int',\n",
       " 'factor',\n",
       " 'float']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import datetime as dt\n",
    "from river import datasets\n",
    "from river import time_series\n",
    "from river import utils\n",
    "import calendar\n",
    "import math\n",
    "from river import compose\n",
    "from river import linear_model\n",
    "from river import optim\n",
    "from river import preprocessing\n",
    "from river import metrics\n",
    "from spotRiver.fun.hyperriver import HyperRiver\n",
    "\n",
    "fun = HyperRiver(123).fun_HTR_iter_progressive\n",
    "var_type = [\"int\"] * 2 + [\"num\"] * 2 + [\"factor\"] * 2 + [\"float\"] + [\"int\"] * 2 + [\"factor\"] + [\"float\"]\n",
    "var_type\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note: Instead of using the Airline Passenger data as in notebook 01, we demonstrate the usage of user specified data which is *not* part of the `spotRiver` package!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Specified Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspotstream.datasets import fetch_opm\n",
    "# ds = fetch_opm(include_categorical=False, data_home=\"data\", return_X_y=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Important: Specify the location of your data, e.g., `/home/data/` etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotRiver.data.generic import GenericData\n",
    "fraction = 1.0\n",
    "dataset = GenericData(filename=\"opm_cat.csv\",\n",
    "                      directory=\".\",\n",
    "                      target=\"Sale Amount\",\n",
    "                      n_features=14,\n",
    "                      n_samples=985_862,\n",
    "                      fraction = fraction,\n",
    "                      converters={'List Year': int,\n",
    "                                  'Assessed Value': float,\n",
    "                                  'Sale Amount': float,\n",
    "                                  'Sales Ratio': float,\n",
    "                                  'lon': float,\n",
    "                                  'lat': float,\n",
    "                                  'timestamp_rec': float},\n",
    "                      parse_dates=None\n",
    "                      # parse_dates={\"Date Recorded\": \"%Y-%m-%d\"}\n",
    "                      )\n",
    "n_samples = int(dataset.n_samples * fraction)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECK Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from river.datasets import synth\n",
    "# dataset = synth.Friedman(seed=42).take(100_000)\n",
    "# n_samples = 500_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'List Year': 2001, 'Assessed Value': 107530.0, 'Sales Ratio': 0.575026738, 'lat': 41.50021401170393, 'lon': -72.87396263711757, 'timestamp_rec': 1001894400.0, 'Town': 'Bristol', 'Address': \"MULTI #'S CONSTANCE LN\", 'Property Type': 'Unknown', 'Residential Type': 'Unknown', 'Non Use Code': 'Unknown', 'Assessor Remarks': 'Unknown', 'OPM remarks': 'Unknown'} 187000.0\n"
     ]
    }
   ],
   "source": [
    "for x,y in dataset:\n",
    "    print(x,y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [[2.00e+02 1.00e+01 1.00e-06 7.50e-02 2.00e+00 1.00e+00 0.00e+00 7.00e+00 0.00e+00 3.12e+02]]\n",
    "lower = np.array([100, 1,  1e-8, 0.025, 0, 1, 0.8,  0,  5, 0, 250.0])\n",
    "upper = np.array([200, 10, 1e-6, 0.075, 2, 1, 0.975, 1, 10, 0, 750.0])\n",
    "fun_control = {\"data\": dataset,\n",
    "               \"n_samples\": n_samples}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Run the `Spot` Optimizer\n",
    "\n",
    "* Since the data is larger than the airline passengers data, the max. time is increased."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Errors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This setting fails:\n",
    "\n",
    "grace_period 105\n",
    "max_depth 1000\n",
    "delta 9.752288533728755e-07\n",
    "tau 0.0691657548549048\n",
    "leaf_prediction adaptive\n",
    "leaf_model Perceptron\n",
    "model_selector_decay 1.0\n",
    "splitter EBSTSplitter\n",
    "min_samples_split 5\n",
    "binary_split 0\n",
    "max_size 564.0\n",
    "Eval iter_prog_val_score: [#---------] 14.20% \n",
    "\n",
    "\n",
    "with the following error messages:\n",
    "\n",
    "grace_period 193\n",
    "max_depth 10000\n",
    "delta 5.160737490382046e-07\n",
    "tau 0.030631256378602662\n",
    "leaf_prediction adaptive\n",
    "leaf_model Perceptron\n",
    "model_selector_decay 1.0\n",
    "splitter QOSplitter\n",
    "min_samples_split 7\n",
    "binary_split 0\n",
    "max_size 354.0\n",
    "Eval iter_prog_val_score: [##########] 100.00% Done...\n",
    "grace_period 136\n",
    "max_depth 10\n",
    "delta 4.358994017976757e-07\n",
    "tau 0.06735151680780518\n",
    "leaf_prediction mean\n",
    "leaf_model Perceptron\n",
    "model_selector_decay 1.0\n",
    "splitter QOSplitter\n",
    "min_samples_split 10\n",
    "binary_split 0\n",
    "max_size 385.0\n",
    "Eval iter_prog_val_score: [##########] 100.00% Done...\n",
    "grace_period 101\n",
    "max_depth 100000000\n",
    "delta 6.740303972572442e-07\n",
    "tau 0.056668192013518505\n",
    "leaf_prediction adaptive\n",
    "leaf_model Perceptron\n",
    "model_selector_decay 1.0\n",
    "splitter EBSTSplitter\n",
    "min_samples_split 6\n",
    "binary_split 0\n",
    "max_size 721.0\n",
    "Eval iter_prog_val_score: [##########] 100.00% Done...\n",
    "grace_period 105\n",
    "max_depth 1000\n",
    "delta 9.752288533728755e-07\n",
    "tau 0.0691657548549048\n",
    "leaf_prediction adaptive\n",
    "leaf_model Perceptron\n",
    "model_selector_decay 1.0\n",
    "splitter EBSTSplitter\n",
    "min_samples_split 5\n",
    "binary_split 0\n",
    "max_size 564.0\n",
    "Eval iter_prog_val_score: [#---------] 14.20% \n",
    "Output exceeds the size limit. Open the full output data in a text editor\n",
    "---------------------------------------------------------------------------\n",
    "RecursionError                            Traceback (most recent call last)\n",
    "Cell In [10], line 27\n",
    "      1 spot_htr = spot.Spot(fun=fun,\n",
    "      2                    lower = lower,\n",
    "      3                    upper = upper,\n",
    "   (...)\n",
    "     25                                       \"model_fun_evals\": 1000,\n",
    "     26                                       })\n",
    "---> 27 spot_htr.run()\n",
    "\n",
    "File ~/miniforge3/envs/spotCondaEnv/lib/python3.10/site-packages/spotPython/spot/spot.py:290, in Spot.run(self)\n",
    "    288 else:\n",
    "    289     X_all = X0\n",
    "--> 290 self.y = self.fun(X=X_all, fun_control=self.fun_control)\n",
    "    291 self.X, self.y = remove_nan(self.X, self.y)\n",
    "    292 self.update_stats()\n",
    "\n",
    "File ~/miniforge3/envs/spotCondaEnv/lib/python3.10/site-packages/spotRiver/fun/hyperriver.py:399, in HyperRiver.fun_HTR_iter_progressive(self, X, fun_control)\n",
    "    397 # cat = compose.SelectType(str) | preprocessing.OneHotEncoder()\n",
    "    398 cat = compose.SelectType(str) | preprocessing.FeatureHasher(n_features=1000, seed=1)\n",
    "--> 399 res = eval_oml_iter_progressive(\n",
    "    400     dataset=self.fun_control[\"data\"],\n",
    "    401     step=10000,\n",
    "    402     verbose=True,\n",
    "    403     metric=metrics.MAE(),\n",
    "    404     models={\n",
    "    405         \"HTR\": (\n",
    "    406             (num + cat)\n",
    "    407             | tree.HoeffdingTreeRegressor(\n",
    "    408                 grace_period=int(grace_period[i]),\n",
    "    409                 max_depth=select_max_depth(int(max_depth[i])),\n",
    "    410                 delta=float(delta[i]),\n",
    "    411                 tau=float(tau[i]),\n",
    "    412                 leaf_prediction=select_leaf_prediction(int(leaf_prediction[i])),\n",
    "    413                 leaf_model=select_leaf_model(int(leaf_model[i])),\n",
    "    414                 model_selector_decay=float(model_selector_decay[i]),\n",
    "    415                 splitter=select_splitter(int(splitter[i])),\n",
    "    416                 min_samples_split=int(min_samples_split[i]),\n",
    "    417                 binary_split=int(binary_split[i]),\n",
    "    418                 max_size=float(max_size[i])\n",
    "    419             )\n",
    "    420         ),\n",
    "    421     },\n",
    "    422 )\n",
    "    423 y = fun_eval_oml_iter_progressive(res, metric=None)\n",
    "    424 z_res = np.append(z_res, y / self.fun_control[\"n_samples\"])\n",
    "\n",
    "File ~/miniforge3/envs/spotCondaEnv/lib/python3.10/site-packages/spotRiver/evaluation/eval_oml.py:28, in eval_oml_iter_progressive(dataset, metric, models, step, verbose)\n",
    "     26 for model_name, model in models.items():\n",
    "     27     result_i = {\"step\": [], \"error\": [], \"r_time\": [], \"memory\": []}\n",
    "---> 28     for checkpoint in iter_progressive_val_score(\n",
    "     29         dataset, model, metric, measure_time=True, measure_memory=True, step=step\n",
    "     30     ):\n",
    "     31         if verbose:\n",
    "     32             progress_bar(checkpoint[\"Step\"] / n_steps, message=\"Eval iter_prog_val_score:\")\n",
    "\n",
    "File ~/miniforge3/envs/spotCondaEnv/lib/python3.10/site-packages/river/evaluate/progressive_validation.py:190, in iter_progressive_val_score(dataset, model, metric, moment, delay, step, measure_time, measure_memory)\n",
    "    102 def iter_progressive_val_score(\n",
    "    103     dataset: base.typing.Dataset,\n",
    "    104     model,\n",
    "   (...)\n",
    "    110     measure_memory=False,\n",
    "    111 ) -> typing.Generator:\n",
    "    112     \"\"\"Evaluates the performance of a model on a streaming dataset and yields results.\n",
    "    113 \n",
    "    114     This does exactly the same as `evaluate.progressive_val_score`. The only difference is that\n",
    "   (...)\n",
    "    187 \n",
    "    188     \"\"\"\n",
    "--> 190     yield from _progressive_validation(\n",
    "    191         dataset,\n",
    "    192         model,\n",
    "    193         metric,\n",
    "    194         checkpoints=itertools.count(step, step) if step else iter([]),\n",
    "    195         moment=moment,\n",
    "    196         delay=delay,\n",
    "    197         measure_time=measure_time,\n",
    "    198         measure_memory=measure_memory,\n",
    "    199     )\n",
    "\n",
    "File ~/miniforge3/envs/spotCondaEnv/lib/python3.10/site-packages/river/evaluate/progressive_validation.py:86, in _progressive_validation(dataset, model, metric, checkpoints, moment, delay, measure_time, measure_memory)\n",
    "     84 n_samples_learned += 1\n",
    "     85 if model._supervised:\n",
    "---> 86     model.learn_one(x, y, **kwargs)\n",
    "     87 else:\n",
    "     88     model.learn_one(x, **kwargs)\n",
    "\n",
    "File ~/miniforge3/envs/spotCondaEnv/lib/python3.10/site-packages/river/compose/pipeline.py:533, in Pipeline.learn_one(self, x, y, **params)\n",
    "    531 last_step = next(steps)\n",
    "    532 if last_step._supervised:\n",
    "--> 533     last_step.learn_one(x=x, y=y, **params)\n",
    "    534 else:\n",
    "    535     last_step.learn_one(x, **params)\n",
    "...\n",
    "--> 137 d = id(x)\n",
    "    138 y = memo.get(d, _nil)\n",
    "    139 if y is not _nil:\n",
    "\n",
    "RecursionError: maximum recursion depth exceeded while calling a Python object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grace_period 193\n",
      "max_depth 10000\n",
      "delta 5.160737490382046e-07\n",
      "tau 0.030631256378602662\n",
      "leaf_prediction adaptive\n",
      "leaf_model Perceptron\n",
      "model_selector_decay 1.0\n",
      "splitter QOSplitter\n",
      "min_samples_split 7\n",
      "binary_split 0\n",
      "max_size 354.0\n",
      "Eval iter_prog_val_score: [##########] 100.00% Done...\n",
      "grace_period 136\n",
      "max_depth 10\n",
      "delta 4.358994017976757e-07\n",
      "tau 0.06735151680780518\n",
      "leaf_prediction mean\n",
      "leaf_model Perceptron\n",
      "model_selector_decay 1.0\n",
      "splitter QOSplitter\n",
      "min_samples_split 10\n",
      "binary_split 0\n",
      "max_size 385.0\n",
      "Eval iter_prog_val_score: [##########] 100.00% Done...\n",
      "grace_period 101\n",
      "max_depth 100000000\n",
      "delta 6.740303972572442e-07\n",
      "tau 0.056668192013518505\n",
      "leaf_prediction adaptive\n",
      "leaf_model Perceptron\n",
      "model_selector_decay 1.0\n",
      "splitter EBSTSplitter\n",
      "min_samples_split 6\n",
      "binary_split 0\n",
      "max_size 721.0\n",
      "Eval iter_prog_val_score: [##########] 100.00% Done...\n",
      "grace_period 105\n",
      "max_depth 1000\n",
      "delta 9.752288533728755e-07\n",
      "tau 0.0691657548549048\n",
      "leaf_prediction adaptive\n",
      "leaf_model Perceptron\n",
      "model_selector_decay 1.0\n",
      "splitter EBSTSplitter\n",
      "min_samples_split 5\n",
      "binary_split 0\n",
      "max_size 564.0\n",
      "Eval iter_prog_val_score: [#---------] 14.20% \r"
     ]
    },
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded while calling a Python object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 27\u001b[0m\n\u001b[1;32m      1\u001b[0m spot_htr \u001b[39m=\u001b[39m spot\u001b[39m.\u001b[39mSpot(fun\u001b[39m=\u001b[39mfun,\n\u001b[1;32m      2\u001b[0m                    lower \u001b[39m=\u001b[39m lower,\n\u001b[1;32m      3\u001b[0m                    upper \u001b[39m=\u001b[39m upper,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m                                       \u001b[39m\"\u001b[39m\u001b[39mmodel_fun_evals\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m1000\u001b[39m,\n\u001b[1;32m     26\u001b[0m                                       })\n\u001b[0;32m---> 27\u001b[0m spot_htr\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/miniforge3/envs/spotCondaEnv/lib/python3.10/site-packages/spotPython/spot/spot.py:290\u001b[0m, in \u001b[0;36mSpot.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    289\u001b[0m     X_all \u001b[39m=\u001b[39m X0\n\u001b[0;32m--> 290\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfun(X\u001b[39m=\u001b[39;49mX_all, fun_control\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfun_control)\n\u001b[1;32m    291\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mX, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my \u001b[39m=\u001b[39m remove_nan(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mX, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my)\n\u001b[1;32m    292\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_stats()\n",
      "File \u001b[0;32m~/miniforge3/envs/spotCondaEnv/lib/python3.10/site-packages/spotRiver/fun/hyperriver.py:399\u001b[0m, in \u001b[0;36mHyperRiver.fun_HTR_iter_progressive\u001b[0;34m(self, X, fun_control)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[39m# cat = compose.SelectType(str) | preprocessing.OneHotEncoder()\u001b[39;00m\n\u001b[1;32m    398\u001b[0m cat \u001b[39m=\u001b[39m compose\u001b[39m.\u001b[39mSelectType(\u001b[39mstr\u001b[39m) \u001b[39m|\u001b[39m preprocessing\u001b[39m.\u001b[39mFeatureHasher(n_features\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m, seed\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 399\u001b[0m res \u001b[39m=\u001b[39m eval_oml_iter_progressive(\n\u001b[1;32m    400\u001b[0m     dataset\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfun_control[\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    401\u001b[0m     step\u001b[39m=\u001b[39;49m\u001b[39m10000\u001b[39;49m,\n\u001b[1;32m    402\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    403\u001b[0m     metric\u001b[39m=\u001b[39;49mmetrics\u001b[39m.\u001b[39;49mMAE(),\n\u001b[1;32m    404\u001b[0m     models\u001b[39m=\u001b[39;49m{\n\u001b[1;32m    405\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mHTR\u001b[39;49m\u001b[39m\"\u001b[39;49m: (\n\u001b[1;32m    406\u001b[0m             (num \u001b[39m+\u001b[39;49m cat)\n\u001b[1;32m    407\u001b[0m             \u001b[39m|\u001b[39;49m tree\u001b[39m.\u001b[39;49mHoeffdingTreeRegressor(\n\u001b[1;32m    408\u001b[0m                 grace_period\u001b[39m=\u001b[39;49m\u001b[39mint\u001b[39;49m(grace_period[i]),\n\u001b[1;32m    409\u001b[0m                 max_depth\u001b[39m=\u001b[39;49mselect_max_depth(\u001b[39mint\u001b[39;49m(max_depth[i])),\n\u001b[1;32m    410\u001b[0m                 delta\u001b[39m=\u001b[39;49m\u001b[39mfloat\u001b[39;49m(delta[i]),\n\u001b[1;32m    411\u001b[0m                 tau\u001b[39m=\u001b[39;49m\u001b[39mfloat\u001b[39;49m(tau[i]),\n\u001b[1;32m    412\u001b[0m                 leaf_prediction\u001b[39m=\u001b[39;49mselect_leaf_prediction(\u001b[39mint\u001b[39;49m(leaf_prediction[i])),\n\u001b[1;32m    413\u001b[0m                 leaf_model\u001b[39m=\u001b[39;49mselect_leaf_model(\u001b[39mint\u001b[39;49m(leaf_model[i])),\n\u001b[1;32m    414\u001b[0m                 model_selector_decay\u001b[39m=\u001b[39;49m\u001b[39mfloat\u001b[39;49m(model_selector_decay[i]),\n\u001b[1;32m    415\u001b[0m                 splitter\u001b[39m=\u001b[39;49mselect_splitter(\u001b[39mint\u001b[39;49m(splitter[i])),\n\u001b[1;32m    416\u001b[0m                 min_samples_split\u001b[39m=\u001b[39;49m\u001b[39mint\u001b[39;49m(min_samples_split[i]),\n\u001b[1;32m    417\u001b[0m                 binary_split\u001b[39m=\u001b[39;49m\u001b[39mint\u001b[39;49m(binary_split[i]),\n\u001b[1;32m    418\u001b[0m                 max_size\u001b[39m=\u001b[39;49m\u001b[39mfloat\u001b[39;49m(max_size[i])\n\u001b[1;32m    419\u001b[0m             )\n\u001b[1;32m    420\u001b[0m         ),\n\u001b[1;32m    421\u001b[0m     },\n\u001b[1;32m    422\u001b[0m )\n\u001b[1;32m    423\u001b[0m y \u001b[39m=\u001b[39m fun_eval_oml_iter_progressive(res, metric\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    424\u001b[0m z_res \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mappend(z_res, y \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfun_control[\u001b[39m\"\u001b[39m\u001b[39mn_samples\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/miniforge3/envs/spotCondaEnv/lib/python3.10/site-packages/spotRiver/evaluation/eval_oml.py:28\u001b[0m, in \u001b[0;36meval_oml_iter_progressive\u001b[0;34m(dataset, metric, models, step, verbose)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mfor\u001b[39;00m model_name, model \u001b[39min\u001b[39;00m models\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     27\u001b[0m     result_i \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m\"\u001b[39m: [], \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m: [], \u001b[39m\"\u001b[39m\u001b[39mr_time\u001b[39m\u001b[39m\"\u001b[39m: [], \u001b[39m\"\u001b[39m\u001b[39mmemory\u001b[39m\u001b[39m\"\u001b[39m: []}\n\u001b[0;32m---> 28\u001b[0m     \u001b[39mfor\u001b[39;00m checkpoint \u001b[39min\u001b[39;00m iter_progressive_val_score(\n\u001b[1;32m     29\u001b[0m         dataset, model, metric, measure_time\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, measure_memory\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, step\u001b[39m=\u001b[39mstep\n\u001b[1;32m     30\u001b[0m     ):\n\u001b[1;32m     31\u001b[0m         \u001b[39mif\u001b[39;00m verbose:\n\u001b[1;32m     32\u001b[0m             progress_bar(checkpoint[\u001b[39m\"\u001b[39m\u001b[39mStep\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m/\u001b[39m n_steps, message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEval iter_prog_val_score:\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/spotCondaEnv/lib/python3.10/site-packages/river/evaluate/progressive_validation.py:190\u001b[0m, in \u001b[0;36miter_progressive_val_score\u001b[0;34m(dataset, model, metric, moment, delay, step, measure_time, measure_memory)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39miter_progressive_val_score\u001b[39m(\n\u001b[1;32m    103\u001b[0m     dataset: base\u001b[39m.\u001b[39mtyping\u001b[39m.\u001b[39mDataset,\n\u001b[1;32m    104\u001b[0m     model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    110\u001b[0m     measure_memory\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    111\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m typing\u001b[39m.\u001b[39mGenerator:\n\u001b[1;32m    112\u001b[0m     \u001b[39m\"\"\"Evaluates the performance of a model on a streaming dataset and yields results.\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \n\u001b[1;32m    114\u001b[0m \u001b[39m    This does exactly the same as `evaluate.progressive_val_score`. The only difference is that\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    187\u001b[0m \n\u001b[1;32m    188\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m     \u001b[39myield from\u001b[39;00m _progressive_validation(\n\u001b[1;32m    191\u001b[0m         dataset,\n\u001b[1;32m    192\u001b[0m         model,\n\u001b[1;32m    193\u001b[0m         metric,\n\u001b[1;32m    194\u001b[0m         checkpoints\u001b[39m=\u001b[39mitertools\u001b[39m.\u001b[39mcount(step, step) \u001b[39mif\u001b[39;00m step \u001b[39melse\u001b[39;00m \u001b[39miter\u001b[39m([]),\n\u001b[1;32m    195\u001b[0m         moment\u001b[39m=\u001b[39mmoment,\n\u001b[1;32m    196\u001b[0m         delay\u001b[39m=\u001b[39mdelay,\n\u001b[1;32m    197\u001b[0m         measure_time\u001b[39m=\u001b[39mmeasure_time,\n\u001b[1;32m    198\u001b[0m         measure_memory\u001b[39m=\u001b[39mmeasure_memory,\n\u001b[1;32m    199\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/spotCondaEnv/lib/python3.10/site-packages/river/evaluate/progressive_validation.py:86\u001b[0m, in \u001b[0;36m_progressive_validation\u001b[0;34m(dataset, model, metric, checkpoints, moment, delay, measure_time, measure_memory)\u001b[0m\n\u001b[1;32m     84\u001b[0m n_samples_learned \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     85\u001b[0m \u001b[39mif\u001b[39;00m model\u001b[39m.\u001b[39m_supervised:\n\u001b[0;32m---> 86\u001b[0m     model\u001b[39m.\u001b[39;49mlearn_one(x, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     87\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m     model\u001b[39m.\u001b[39mlearn_one(x, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/spotCondaEnv/lib/python3.10/site-packages/river/compose/pipeline.py:533\u001b[0m, in \u001b[0;36mPipeline.learn_one\u001b[0;34m(self, x, y, **params)\u001b[0m\n\u001b[1;32m    531\u001b[0m last_step \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(steps)\n\u001b[1;32m    532\u001b[0m \u001b[39mif\u001b[39;00m last_step\u001b[39m.\u001b[39m_supervised:\n\u001b[0;32m--> 533\u001b[0m     last_step\u001b[39m.\u001b[39;49mlearn_one(x\u001b[39m=\u001b[39;49mx, y\u001b[39m=\u001b[39;49my, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    534\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    535\u001b[0m     last_step\u001b[39m.\u001b[39mlearn_one(x, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n",
      "File \u001b[0;32m~/miniforge3/envs/spotCondaEnv/lib/python3.10/site-packages/river/tree/hoeffding_tree_regressor.py:265\u001b[0m, in \u001b[0;36mHoeffdingTreeRegressor.learn_one\u001b[0;34m(self, x, y, sample_weight)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[39mif\u001b[39;00m weight_diff \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrace_period:\n\u001b[1;32m    264\u001b[0m                 p_branch \u001b[39m=\u001b[39m p_node\u001b[39m.\u001b[39mbranch_no(x) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(p_node, DTBranch) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attempt_to_split(node, p_node, p_branch)\n\u001b[1;32m    266\u001b[0m                 node\u001b[39m.\u001b[39mlast_split_attempt_at \u001b[39m=\u001b[39m weight_seen\n\u001b[1;32m    267\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/spotCondaEnv/lib/python3.10/site-packages/river/tree/hoeffding_tree_regressor.py:347\u001b[0m, in \u001b[0;36mHoeffdingTreeRegressor._attempt_to_split\u001b[0;34m(self, leaf, parent, parent_branch, **kwargs)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[39m\"\"\"Attempt to split a node.\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \n\u001b[1;32m    321\u001b[0m \u001b[39mIf the target's variance is high at the leaf node, then:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    344\u001b[0m \n\u001b[1;32m    345\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    346\u001b[0m split_criterion \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_new_split_criterion()\n\u001b[0;32m--> 347\u001b[0m best_split_suggestions \u001b[39m=\u001b[39m leaf\u001b[39m.\u001b[39;49mbest_split_suggestions(split_criterion, \u001b[39mself\u001b[39;49m)\n\u001b[1;32m    348\u001b[0m best_split_suggestions\u001b[39m.\u001b[39msort()\n\u001b[1;32m    349\u001b[0m should_split \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/spotCondaEnv/lib/python3.10/site-packages/river/tree/nodes/leaf.py:130\u001b[0m, in \u001b[0;36mHTLeaf.best_split_suggestions\u001b[0;34m(self, criterion, tree)\u001b[0m\n\u001b[1;32m    128\u001b[0m     best_suggestions\u001b[39m.\u001b[39mappend(null_split)\n\u001b[1;32m    129\u001b[0m \u001b[39mfor\u001b[39;00m att_id, splitter \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplitters\u001b[39m.\u001b[39mitems():\n\u001b[0;32m--> 130\u001b[0m     best_suggestion \u001b[39m=\u001b[39m splitter\u001b[39m.\u001b[39;49mbest_evaluated_split_suggestion(\n\u001b[1;32m    131\u001b[0m         criterion, pre_split_dist, att_id, tree\u001b[39m.\u001b[39;49mbinary_split\n\u001b[1;32m    132\u001b[0m     )\n\u001b[1;32m    133\u001b[0m     best_suggestions\u001b[39m.\u001b[39mappend(best_suggestion)\n\u001b[1;32m    135\u001b[0m \u001b[39mreturn\u001b[39;00m best_suggestions\n",
      "File \u001b[0;32m~/miniforge3/envs/spotCondaEnv/lib/python3.10/site-packages/river/tree/splitter/ebst_splitter.py:77\u001b[0m, in \u001b[0;36mEBSTSplitter.best_evaluated_split_suggestion\u001b[0;34m(self, criterion, pre_split_dist, att_idx, binary_only)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_aux_estimator \u001b[39m=\u001b[39m Var()\n\u001b[0;32m---> 77\u001b[0m best_split \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_find_best_split(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_root, candidate)\n\u001b[1;32m     79\u001b[0m \u001b[39m# Delete auxiliary variables\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_criterion\n",
      "File \u001b[0;32m~/miniforge3/envs/spotCondaEnv/lib/python3.10/site-packages/river/tree/splitter/ebst_splitter.py:106\u001b[0m, in \u001b[0;36mEBSTSplitter._find_best_split\u001b[0;34m(self, node, candidate)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mif\u001b[39;00m node\u001b[39m.\u001b[39m_right \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_aux_estimator \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mestimator\n\u001b[0;32m--> 106\u001b[0m     right_candidate \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_find_best_split(node\u001b[39m.\u001b[39;49m_right, candidate)\n\u001b[1;32m    108\u001b[0m     \u001b[39mif\u001b[39;00m right_candidate\u001b[39m.\u001b[39mmerit \u001b[39m>\u001b[39m candidate\u001b[39m.\u001b[39mmerit:\n\u001b[1;32m    109\u001b[0m         candidate \u001b[39m=\u001b[39m right_candidate\n",
      "File \u001b[0;32m~/miniforge3/envs/spotCondaEnv/lib/python3.10/site-packages/river/tree/splitter/ebst_splitter.py:106\u001b[0m, in \u001b[0;36mEBSTSplitter._find_best_split\u001b[0;34m(self, node, candidate)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mif\u001b[39;00m node\u001b[39m.\u001b[39m_right \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_aux_estimator \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mestimator\n\u001b[0;32m--> 106\u001b[0m     right_candidate \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_find_best_split(node\u001b[39m.\u001b[39;49m_right, candidate)\n\u001b[1;32m    108\u001b[0m     \u001b[39mif\u001b[39;00m right_candidate\u001b[39m.\u001b[39mmerit \u001b[39m>\u001b[39m candidate\u001b[39m.\u001b[39mmerit:\n\u001b[1;32m    109\u001b[0m         candidate \u001b[39m=\u001b[39m right_candidate\n",
      "    \u001b[0;31m[... skipping similar frames: EBSTSplitter._find_best_split at line 106 (2949 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/spotCondaEnv/lib/python3.10/site-packages/river/tree/splitter/ebst_splitter.py:106\u001b[0m, in \u001b[0;36mEBSTSplitter._find_best_split\u001b[0;34m(self, node, candidate)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mif\u001b[39;00m node\u001b[39m.\u001b[39m_right \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_aux_estimator \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mestimator\n\u001b[0;32m--> 106\u001b[0m     right_candidate \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_find_best_split(node\u001b[39m.\u001b[39;49m_right, candidate)\n\u001b[1;32m    108\u001b[0m     \u001b[39mif\u001b[39;00m right_candidate\u001b[39m.\u001b[39mmerit \u001b[39m>\u001b[39m candidate\u001b[39m.\u001b[39mmerit:\n\u001b[1;32m    109\u001b[0m         candidate \u001b[39m=\u001b[39m right_candidate\n",
      "File \u001b[0;32m~/miniforge3/envs/spotCondaEnv/lib/python3.10/site-packages/river/tree/splitter/ebst_splitter.py:91\u001b[0m, in \u001b[0;36mEBSTSplitter._find_best_split\u001b[0;34m(self, node, candidate)\u001b[0m\n\u001b[1;32m     89\u001b[0m     candidate \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_find_best_split(node\u001b[39m.\u001b[39m_left, candidate)\n\u001b[1;32m     90\u001b[0m \u001b[39m# Left post split distribution\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m left_dist \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39;49mestimator \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_aux_estimator\n\u001b[1;32m     93\u001b[0m \u001b[39m# The right split distribution is calculated as the difference between the total\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[39m# distribution (pre split distribution) and the left distribution\u001b[39;00m\n\u001b[1;32m     95\u001b[0m right_dist \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pre_split_dist \u001b[39m-\u001b[39m left_dist\n",
      "File \u001b[0;32m~/miniforge3/envs/spotCondaEnv/lib/python3.10/site-packages/river/stats/var.py:124\u001b[0m, in \u001b[0;36mVar.__add__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__add__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[0;32m--> 124\u001b[0m     result \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39;49mdeepcopy(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    125\u001b[0m     result \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m other\n\u001b[1;32m    126\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniforge3/envs/spotCondaEnv/lib/python3.10/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[39m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[39m=\u001b[39m _reconstruct(x, memo, \u001b[39m*\u001b[39;49mrv)\n\u001b[1;32m    174\u001b[0m \u001b[39m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m~/miniforge3/envs/spotCondaEnv/lib/python3.10/copy.py:271\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m deep:\n\u001b[0;32m--> 271\u001b[0m         state \u001b[39m=\u001b[39m deepcopy(state, memo)\n\u001b[1;32m    272\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(y, \u001b[39m'\u001b[39m\u001b[39m__setstate__\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    273\u001b[0m         y\u001b[39m.\u001b[39m__setstate__(state)\n",
      "File \u001b[0;32m~/miniforge3/envs/spotCondaEnv/lib/python3.10/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[39m=\u001b[39m _deepcopy_dispatch\u001b[39m.\u001b[39mget(\u001b[39mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[39mif\u001b[39;00m copier \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[39m=\u001b[39m copier(x, memo)\n\u001b[1;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39mtype\u001b[39m):\n",
      "File \u001b[0;32m~/miniforge3/envs/spotCondaEnv/lib/python3.10/copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    229\u001b[0m memo[\u001b[39mid\u001b[39m(x)] \u001b[39m=\u001b[39m y\n\u001b[1;32m    230\u001b[0m \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m x\u001b[39m.\u001b[39mitems():\n\u001b[0;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[39m=\u001b[39m deepcopy(value, memo)\n\u001b[1;32m    232\u001b[0m \u001b[39mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/miniforge3/envs/spotCondaEnv/lib/python3.10/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[39m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[39m=\u001b[39m _reconstruct(x, memo, \u001b[39m*\u001b[39;49mrv)\n\u001b[1;32m    174\u001b[0m \u001b[39m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m~/miniforge3/envs/spotCondaEnv/lib/python3.10/copy.py:271\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m deep:\n\u001b[0;32m--> 271\u001b[0m         state \u001b[39m=\u001b[39m deepcopy(state, memo)\n\u001b[1;32m    272\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(y, \u001b[39m'\u001b[39m\u001b[39m__setstate__\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    273\u001b[0m         y\u001b[39m.\u001b[39m__setstate__(state)\n",
      "File \u001b[0;32m~/miniforge3/envs/spotCondaEnv/lib/python3.10/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[39m=\u001b[39m _deepcopy_dispatch\u001b[39m.\u001b[39mget(\u001b[39mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[39mif\u001b[39;00m copier \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[39m=\u001b[39m copier(x, memo)\n\u001b[1;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39mtype\u001b[39m):\n",
      "File \u001b[0;32m~/miniforge3/envs/spotCondaEnv/lib/python3.10/copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    229\u001b[0m memo[\u001b[39mid\u001b[39m(x)] \u001b[39m=\u001b[39m y\n\u001b[1;32m    230\u001b[0m \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m x\u001b[39m.\u001b[39mitems():\n\u001b[0;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[39m=\u001b[39m deepcopy(value, memo)\n\u001b[1;32m    232\u001b[0m \u001b[39mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/miniforge3/envs/spotCondaEnv/lib/python3.10/copy.py:137\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39mif\u001b[39;00m memo \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     memo \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 137\u001b[0m d \u001b[39m=\u001b[39m \u001b[39mid\u001b[39;49m(x)\n\u001b[1;32m    138\u001b[0m y \u001b[39m=\u001b[39m memo\u001b[39m.\u001b[39mget(d, _nil)\n\u001b[1;32m    139\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m _nil:\n",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded while calling a Python object"
     ]
    }
   ],
   "source": [
    "spot_htr = spot.Spot(fun=fun,\n",
    "                   lower = lower,\n",
    "                   upper = upper,\n",
    "                   fun_evals = inf,\n",
    "                   fun_repeats = 1,\n",
    "                   max_time = 300,\n",
    "                   noise = False,\n",
    "                   tolerance_x = np.sqrt(np.spacing(1)),\n",
    "                   var_type=var_type,\n",
    "                   infill_criterion = \"y\",\n",
    "                   n_points = 1,\n",
    "                   seed=123,\n",
    "                   log_level = 50,\n",
    "                   show_models= False,\n",
    "                   show_progress= True,\n",
    "                   fun_control = fun_control,\n",
    "                   design_control={\"init_size\": 25,\n",
    "                                   \"repeats\": 1},\n",
    "                   surrogate_control={\"noise\": False,\n",
    "                                      \"cod_type\": \"norm\",\n",
    "                                      \"min_theta\": -4,\n",
    "                                      \"max_theta\": 3,\n",
    "                                      \"n_theta\": 11,\n",
    "                                      \"model_optimizer\": differential_evolution,\n",
    "                                      \"model_fun_evals\": 1000,\n",
    "                                      })\n",
    "spot_htr.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Results \n",
    "\n",
    "* Print the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_htr.print_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Show the Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_htr.plot_progress(log_y=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_htr.print_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = spot_htr.k\n",
    "for i in range(n-1):\n",
    "    for j in range(i+1, n):\n",
    "        spot_htr.plot_contour(i=i, j=j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Evaluate SNARIMAX Model with Tuned Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = spot_htr.to_all_dim(spot_htr.min_X.reshape(1,-1))\n",
    "print(X)\n",
    "grace_period = X[:, 0]\n",
    "max_depth = X[:, 1]\n",
    "delta = X[:, 2]\n",
    "tau = X[:, 3]\n",
    "leaf_prediction = X[:, 4]\n",
    "leaf_model = X[:, 5]\n",
    "model_selector_decay = X[:, 6]\n",
    "splitter = X[:, 7]\n",
    "min_samples_split = X[:, 8]\n",
    "binary_split = X[:, 9]\n",
    "max_size = X[:, 10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numbers\n",
    "from river import tree\n",
    "from spotRiver.evaluation.eval_oml import fun_eval_oml_iter_progressive\n",
    "from spotRiver.evaluation.eval_oml import eval_oml_iter_progressive\n",
    "from spotRiver.utils.selectors import select_splitter\n",
    "from spotRiver.utils.selectors import select_max_depth\n",
    "num = compose.SelectType(numbers.Number) | preprocessing.StandardScaler()\n",
    "cat = compose.SelectType(str) | preprocessing.FeatureHasher(n_features=1000, seed=1)\n",
    "res = eval_oml_iter_progressive(\n",
    "    dataset=fun_control[\"data\"],\n",
    "    step=10000,\n",
    "    verbose=True,\n",
    "    metric=metrics.MAE(),\n",
    "    models={\n",
    "         \"Default: HTR + QO\": (\n",
    "             (num + cat) | tree.HoeffdingTreeRegressor(\n",
    "                splitter=tree.splitter.QOSplitter()\n",
    "            )\n",
    "        ),\n",
    "        \"SPOT: HTR + QO\": (\n",
    "            (num + cat) | tree.HoeffdingTreeRegressor(\n",
    "                grace_period=int(grace_period),\n",
    "                max_depth=select_max_depth(int(max_depth)),\n",
    "                delta=float(delta),\n",
    "                tau=float(tau),\n",
    "                leaf_prediction=select_leaf_prediction(int(leaf_prediction)),\n",
    "                leaf_model=select_leaf_model(int(leaf_model)),\n",
    "                splitter=select_splitter(int(splitter)),\n",
    "                min_samples_split=int(min_samples_split),\n",
    "                binary_split=int(binary_split),\n",
    "                max_size=float(max_size)\n",
    "            )\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "y = fun_eval_oml_iter_progressive(res, metric=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotRiver.evaluation.eval_oml import plot_oml_iter_progressive\n",
    "plot_oml_iter_progressive(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "# model[\"ts\"].regressor[\"lm\"].weights\n",
    "# forecast = model.forecast(horizon=fun_control[\"horizon\"])\n",
    "# forecast\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spotCondaEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "81c77de872def749acd68d9955e19f0df6803301f4c1f66c3444af66334112ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
